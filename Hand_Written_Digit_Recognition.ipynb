{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-21T14:22:13.557926Z",
     "start_time": "2024-02-21T14:22:06.593223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118282 (462.04 KB)\n",
      "Trainable params: 118282 (462.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 1.3477 - accuracy: 0.6682 - val_loss: 0.6829 - val_accuracy: 0.8424\n",
      "Epoch 2/20\n",
      "375/375 [==============================] - 0s 845us/step - loss: 0.5662 - accuracy: 0.8567 - val_loss: 0.4373 - val_accuracy: 0.8869\n",
      "Epoch 3/20\n",
      "375/375 [==============================] - 0s 865us/step - loss: 0.4275 - accuracy: 0.8849 - val_loss: 0.3656 - val_accuracy: 0.9013\n",
      "Epoch 4/20\n",
      "375/375 [==============================] - 0s 841us/step - loss: 0.3721 - accuracy: 0.8968 - val_loss: 0.3294 - val_accuracy: 0.9068\n",
      "Epoch 5/20\n",
      "375/375 [==============================] - 0s 854us/step - loss: 0.3405 - accuracy: 0.9035 - val_loss: 0.3055 - val_accuracy: 0.9130\n",
      "Epoch 6/20\n",
      "375/375 [==============================] - 0s 875us/step - loss: 0.3182 - accuracy: 0.9092 - val_loss: 0.2887 - val_accuracy: 0.9173\n",
      "Epoch 7/20\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.3009 - accuracy: 0.9145 - val_loss: 0.2757 - val_accuracy: 0.9215\n",
      "Epoch 8/20\n",
      "375/375 [==============================] - 0s 846us/step - loss: 0.2864 - accuracy: 0.9184 - val_loss: 0.2655 - val_accuracy: 0.9238\n",
      "Epoch 9/20\n",
      "375/375 [==============================] - 0s 840us/step - loss: 0.2737 - accuracy: 0.9217 - val_loss: 0.2553 - val_accuracy: 0.9277\n",
      "Epoch 10/20\n",
      "375/375 [==============================] - 0s 836us/step - loss: 0.2629 - accuracy: 0.9252 - val_loss: 0.2466 - val_accuracy: 0.9295\n",
      "Epoch 11/20\n",
      "375/375 [==============================] - 0s 848us/step - loss: 0.2529 - accuracy: 0.9285 - val_loss: 0.2388 - val_accuracy: 0.9325\n",
      "Epoch 12/20\n",
      "375/375 [==============================] - 0s 839us/step - loss: 0.2437 - accuracy: 0.9305 - val_loss: 0.2318 - val_accuracy: 0.9340\n",
      "Epoch 13/20\n",
      "375/375 [==============================] - 0s 841us/step - loss: 0.2354 - accuracy: 0.9335 - val_loss: 0.2244 - val_accuracy: 0.9362\n",
      "Epoch 14/20\n",
      "375/375 [==============================] - 0s 873us/step - loss: 0.2277 - accuracy: 0.9353 - val_loss: 0.2196 - val_accuracy: 0.9382\n",
      "Epoch 15/20\n",
      "375/375 [==============================] - 0s 860us/step - loss: 0.2206 - accuracy: 0.9374 - val_loss: 0.2126 - val_accuracy: 0.9403\n",
      "Epoch 16/20\n",
      "375/375 [==============================] - 0s 851us/step - loss: 0.2138 - accuracy: 0.9396 - val_loss: 0.2072 - val_accuracy: 0.9417\n",
      "Epoch 17/20\n",
      "375/375 [==============================] - 0s 852us/step - loss: 0.2074 - accuracy: 0.9418 - val_loss: 0.2034 - val_accuracy: 0.9433\n",
      "Epoch 18/20\n",
      "375/375 [==============================] - 0s 855us/step - loss: 0.2013 - accuracy: 0.9429 - val_loss: 0.2001 - val_accuracy: 0.9440\n",
      "Epoch 19/20\n",
      "375/375 [==============================] - 0s 838us/step - loss: 0.1960 - accuracy: 0.9447 - val_loss: 0.1933 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "375/375 [==============================] - 0s 841us/step - loss: 0.1905 - accuracy: 0.9463 - val_loss: 0.1895 - val_accuracy: 0.9473\n",
      "313/313 [==============================] - 0s 328us/step - loss: 0.1906 - accuracy: 0.9461\n",
      "Test score: 0.19061198830604553\n",
      "Test accuracy: 0.9460999965667725\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T23:44:28.190086Z",
     "start_time": "2024-02-26T23:44:04.165513Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118282 (462.04 KB)\n",
      "Trainable params: 118282 (462.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "375/375 [==============================] - 1s 1ms/step - loss: 1.5307 - accuracy: 0.6099 - val_loss: 0.7833 - val_accuracy: 0.8398\n",
      "Epoch 2/60\n",
      "375/375 [==============================] - 0s 885us/step - loss: 0.6066 - accuracy: 0.8501 - val_loss: 0.4536 - val_accuracy: 0.8861\n",
      "Epoch 3/60\n",
      "375/375 [==============================] - 0s 892us/step - loss: 0.4357 - accuracy: 0.8813 - val_loss: 0.3710 - val_accuracy: 0.9000\n",
      "Epoch 4/60\n",
      "375/375 [==============================] - 0s 891us/step - loss: 0.3757 - accuracy: 0.8945 - val_loss: 0.3334 - val_accuracy: 0.9070\n",
      "Epoch 5/60\n",
      "375/375 [==============================] - 0s 891us/step - loss: 0.3426 - accuracy: 0.9024 - val_loss: 0.3100 - val_accuracy: 0.9109\n",
      "Epoch 6/60\n",
      "375/375 [==============================] - 0s 882us/step - loss: 0.3200 - accuracy: 0.9088 - val_loss: 0.2933 - val_accuracy: 0.9176\n",
      "Epoch 7/60\n",
      "375/375 [==============================] - 0s 892us/step - loss: 0.3024 - accuracy: 0.9136 - val_loss: 0.2803 - val_accuracy: 0.9207\n",
      "Epoch 8/60\n",
      "375/375 [==============================] - 0s 903us/step - loss: 0.2882 - accuracy: 0.9182 - val_loss: 0.2672 - val_accuracy: 0.9258\n",
      "Epoch 9/60\n",
      "375/375 [==============================] - 0s 917us/step - loss: 0.2756 - accuracy: 0.9220 - val_loss: 0.2600 - val_accuracy: 0.9258\n",
      "Epoch 10/60\n",
      "375/375 [==============================] - 0s 920us/step - loss: 0.2646 - accuracy: 0.9243 - val_loss: 0.2489 - val_accuracy: 0.9299\n",
      "Epoch 11/60\n",
      "375/375 [==============================] - 0s 883us/step - loss: 0.2543 - accuracy: 0.9276 - val_loss: 0.2407 - val_accuracy: 0.9328\n",
      "Epoch 12/60\n",
      "375/375 [==============================] - 0s 917us/step - loss: 0.2454 - accuracy: 0.9298 - val_loss: 0.2336 - val_accuracy: 0.9328\n",
      "Epoch 13/60\n",
      "375/375 [==============================] - 0s 924us/step - loss: 0.2370 - accuracy: 0.9325 - val_loss: 0.2289 - val_accuracy: 0.9346\n",
      "Epoch 14/60\n",
      "375/375 [==============================] - 0s 896us/step - loss: 0.2290 - accuracy: 0.9345 - val_loss: 0.2192 - val_accuracy: 0.9373\n",
      "Epoch 15/60\n",
      "375/375 [==============================] - 0s 932us/step - loss: 0.2218 - accuracy: 0.9360 - val_loss: 0.2133 - val_accuracy: 0.9399\n",
      "Epoch 16/60\n",
      "375/375 [==============================] - 0s 896us/step - loss: 0.2153 - accuracy: 0.9378 - val_loss: 0.2093 - val_accuracy: 0.9408\n",
      "Epoch 17/60\n",
      "375/375 [==============================] - 0s 911us/step - loss: 0.2089 - accuracy: 0.9400 - val_loss: 0.2030 - val_accuracy: 0.9436\n",
      "Epoch 18/60\n",
      "375/375 [==============================] - 0s 866us/step - loss: 0.2029 - accuracy: 0.9414 - val_loss: 0.1992 - val_accuracy: 0.9437\n",
      "Epoch 19/60\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.1974 - accuracy: 0.9437 - val_loss: 0.1942 - val_accuracy: 0.9455\n",
      "Epoch 20/60\n",
      "375/375 [==============================] - 0s 901us/step - loss: 0.1922 - accuracy: 0.9448 - val_loss: 0.1915 - val_accuracy: 0.9456\n",
      "Epoch 21/60\n",
      "375/375 [==============================] - 0s 875us/step - loss: 0.1872 - accuracy: 0.9466 - val_loss: 0.1867 - val_accuracy: 0.9467\n",
      "Epoch 22/60\n",
      "375/375 [==============================] - 0s 883us/step - loss: 0.1825 - accuracy: 0.9481 - val_loss: 0.1830 - val_accuracy: 0.9475\n",
      "Epoch 23/60\n",
      "375/375 [==============================] - 0s 909us/step - loss: 0.1778 - accuracy: 0.9494 - val_loss: 0.1794 - val_accuracy: 0.9496\n",
      "Epoch 24/60\n",
      "375/375 [==============================] - 0s 889us/step - loss: 0.1736 - accuracy: 0.9508 - val_loss: 0.1756 - val_accuracy: 0.9500\n",
      "Epoch 25/60\n",
      "375/375 [==============================] - 0s 895us/step - loss: 0.1695 - accuracy: 0.9518 - val_loss: 0.1725 - val_accuracy: 0.9513\n",
      "Epoch 26/60\n",
      "375/375 [==============================] - 0s 894us/step - loss: 0.1655 - accuracy: 0.9531 - val_loss: 0.1696 - val_accuracy: 0.9525\n",
      "Epoch 27/60\n",
      "375/375 [==============================] - 0s 883us/step - loss: 0.1618 - accuracy: 0.9540 - val_loss: 0.1676 - val_accuracy: 0.9531\n",
      "Epoch 28/60\n",
      "375/375 [==============================] - 0s 913us/step - loss: 0.1579 - accuracy: 0.9553 - val_loss: 0.1652 - val_accuracy: 0.9530\n",
      "Epoch 29/60\n",
      "375/375 [==============================] - 0s 893us/step - loss: 0.1547 - accuracy: 0.9562 - val_loss: 0.1619 - val_accuracy: 0.9546\n",
      "Epoch 30/60\n",
      "375/375 [==============================] - 0s 883us/step - loss: 0.1513 - accuracy: 0.9570 - val_loss: 0.1586 - val_accuracy: 0.9557\n",
      "Epoch 31/60\n",
      "375/375 [==============================] - 0s 898us/step - loss: 0.1481 - accuracy: 0.9585 - val_loss: 0.1563 - val_accuracy: 0.9570\n",
      "Epoch 32/60\n",
      "375/375 [==============================] - 0s 890us/step - loss: 0.1449 - accuracy: 0.9593 - val_loss: 0.1546 - val_accuracy: 0.9558\n",
      "Epoch 33/60\n",
      "375/375 [==============================] - 0s 902us/step - loss: 0.1418 - accuracy: 0.9597 - val_loss: 0.1525 - val_accuracy: 0.9570\n",
      "Epoch 34/60\n",
      "375/375 [==============================] - 0s 875us/step - loss: 0.1389 - accuracy: 0.9609 - val_loss: 0.1498 - val_accuracy: 0.9577\n",
      "Epoch 35/60\n",
      "375/375 [==============================] - 0s 869us/step - loss: 0.1364 - accuracy: 0.9613 - val_loss: 0.1485 - val_accuracy: 0.9570\n",
      "Epoch 36/60\n",
      "375/375 [==============================] - 0s 886us/step - loss: 0.1336 - accuracy: 0.9626 - val_loss: 0.1476 - val_accuracy: 0.9578\n",
      "Epoch 37/60\n",
      "375/375 [==============================] - 0s 889us/step - loss: 0.1311 - accuracy: 0.9631 - val_loss: 0.1436 - val_accuracy: 0.9591\n",
      "Epoch 38/60\n",
      "375/375 [==============================] - 0s 885us/step - loss: 0.1282 - accuracy: 0.9642 - val_loss: 0.1425 - val_accuracy: 0.9582\n",
      "Epoch 39/60\n",
      "375/375 [==============================] - 0s 878us/step - loss: 0.1257 - accuracy: 0.9649 - val_loss: 0.1411 - val_accuracy: 0.9597\n",
      "Epoch 40/60\n",
      "375/375 [==============================] - 0s 895us/step - loss: 0.1235 - accuracy: 0.9653 - val_loss: 0.1387 - val_accuracy: 0.9599\n",
      "Epoch 41/60\n",
      "375/375 [==============================] - 0s 892us/step - loss: 0.1213 - accuracy: 0.9660 - val_loss: 0.1370 - val_accuracy: 0.9609\n",
      "Epoch 42/60\n",
      "375/375 [==============================] - 0s 880us/step - loss: 0.1192 - accuracy: 0.9670 - val_loss: 0.1369 - val_accuracy: 0.9607\n",
      "Epoch 43/60\n",
      "375/375 [==============================] - 0s 877us/step - loss: 0.1168 - accuracy: 0.9678 - val_loss: 0.1353 - val_accuracy: 0.9607\n",
      "Epoch 44/60\n",
      "375/375 [==============================] - 0s 890us/step - loss: 0.1149 - accuracy: 0.9677 - val_loss: 0.1332 - val_accuracy: 0.9613\n",
      "Epoch 45/60\n",
      "375/375 [==============================] - 0s 919us/step - loss: 0.1127 - accuracy: 0.9686 - val_loss: 0.1312 - val_accuracy: 0.9612\n",
      "Epoch 46/60\n",
      "375/375 [==============================] - 0s 909us/step - loss: 0.1106 - accuracy: 0.9693 - val_loss: 0.1308 - val_accuracy: 0.9623\n",
      "Epoch 47/60\n",
      "375/375 [==============================] - 0s 904us/step - loss: 0.1089 - accuracy: 0.9700 - val_loss: 0.1293 - val_accuracy: 0.9622\n",
      "Epoch 48/60\n",
      "375/375 [==============================] - 0s 882us/step - loss: 0.1068 - accuracy: 0.9703 - val_loss: 0.1277 - val_accuracy: 0.9625\n",
      "Epoch 49/60\n",
      "375/375 [==============================] - 0s 886us/step - loss: 0.1051 - accuracy: 0.9708 - val_loss: 0.1266 - val_accuracy: 0.9624\n",
      "Epoch 50/60\n",
      "375/375 [==============================] - 0s 890us/step - loss: 0.1033 - accuracy: 0.9714 - val_loss: 0.1254 - val_accuracy: 0.9637\n",
      "Epoch 51/60\n",
      "375/375 [==============================] - 0s 898us/step - loss: 0.1015 - accuracy: 0.9721 - val_loss: 0.1246 - val_accuracy: 0.9628\n",
      "Epoch 52/60\n",
      "375/375 [==============================] - 0s 890us/step - loss: 0.0998 - accuracy: 0.9727 - val_loss: 0.1246 - val_accuracy: 0.9631\n",
      "Epoch 53/60\n",
      "375/375 [==============================] - 0s 892us/step - loss: 0.0982 - accuracy: 0.9728 - val_loss: 0.1220 - val_accuracy: 0.9641\n",
      "Epoch 54/60\n",
      "375/375 [==============================] - 0s 883us/step - loss: 0.0966 - accuracy: 0.9735 - val_loss: 0.1217 - val_accuracy: 0.9646\n",
      "Epoch 55/60\n",
      "375/375 [==============================] - 0s 876us/step - loss: 0.0950 - accuracy: 0.9739 - val_loss: 0.1207 - val_accuracy: 0.9643\n",
      "Epoch 56/60\n",
      "375/375 [==============================] - 0s 878us/step - loss: 0.0935 - accuracy: 0.9745 - val_loss: 0.1188 - val_accuracy: 0.9652\n",
      "Epoch 57/60\n",
      "375/375 [==============================] - 0s 879us/step - loss: 0.0920 - accuracy: 0.9747 - val_loss: 0.1188 - val_accuracy: 0.9649\n",
      "Epoch 58/60\n",
      "375/375 [==============================] - 0s 906us/step - loss: 0.0908 - accuracy: 0.9753 - val_loss: 0.1172 - val_accuracy: 0.9657\n",
      "Epoch 59/60\n",
      "375/375 [==============================] - 0s 900us/step - loss: 0.0891 - accuracy: 0.9757 - val_loss: 0.1159 - val_accuracy: 0.9669\n",
      "Epoch 60/60\n",
      "375/375 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9756 - val_loss: 0.1162 - val_accuracy: 0.9657\n",
      "313/313 [==============================] - 0s 343us/step - loss: 0.1082 - accuracy: 0.9674\n",
      "Test score: 0.10819873213768005\n",
      "Test accuracy: 0.9674000144004822\n"
     ]
    }
   ],
   "source": [
    "# increasing epochs from 20 to 60\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 60\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T23:51:36.978440Z",
     "start_time": "2024-02-26T23:50:56.793978Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118282 (462.04 KB)\n",
      "Trainable params: 118282 (462.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n",
      "375/375 [==============================] - 0s 989us/step - loss: 1.3672 - accuracy: 0.6593 - val_loss: 0.6778 - val_accuracy: 0.8432\n",
      "Epoch 2/120\n",
      "375/375 [==============================] - 0s 872us/step - loss: 0.5640 - accuracy: 0.8574 - val_loss: 0.4340 - val_accuracy: 0.8878\n",
      "Epoch 3/120\n",
      "375/375 [==============================] - 0s 880us/step - loss: 0.4262 - accuracy: 0.8839 - val_loss: 0.3652 - val_accuracy: 0.9019\n",
      "Epoch 4/120\n",
      "375/375 [==============================] - 0s 883us/step - loss: 0.3720 - accuracy: 0.8960 - val_loss: 0.3299 - val_accuracy: 0.9088\n",
      "Epoch 5/120\n",
      "375/375 [==============================] - 0s 883us/step - loss: 0.3400 - accuracy: 0.9044 - val_loss: 0.3068 - val_accuracy: 0.9142\n",
      "Epoch 6/120\n",
      "375/375 [==============================] - 0s 874us/step - loss: 0.3172 - accuracy: 0.9094 - val_loss: 0.2899 - val_accuracy: 0.9173\n",
      "Epoch 7/120\n",
      "375/375 [==============================] - 0s 882us/step - loss: 0.2997 - accuracy: 0.9140 - val_loss: 0.2762 - val_accuracy: 0.9212\n",
      "Epoch 8/120\n",
      "375/375 [==============================] - 0s 877us/step - loss: 0.2847 - accuracy: 0.9189 - val_loss: 0.2635 - val_accuracy: 0.9256\n",
      "Epoch 9/120\n",
      "375/375 [==============================] - 0s 879us/step - loss: 0.2719 - accuracy: 0.9217 - val_loss: 0.2536 - val_accuracy: 0.9285\n",
      "Epoch 10/120\n",
      "375/375 [==============================] - 0s 879us/step - loss: 0.2607 - accuracy: 0.9245 - val_loss: 0.2447 - val_accuracy: 0.9305\n",
      "Epoch 11/120\n",
      "375/375 [==============================] - 0s 893us/step - loss: 0.2506 - accuracy: 0.9281 - val_loss: 0.2358 - val_accuracy: 0.9333\n",
      "Epoch 12/120\n",
      "375/375 [==============================] - 0s 876us/step - loss: 0.2414 - accuracy: 0.9311 - val_loss: 0.2297 - val_accuracy: 0.9348\n",
      "Epoch 13/120\n",
      "375/375 [==============================] - 0s 874us/step - loss: 0.2328 - accuracy: 0.9331 - val_loss: 0.2219 - val_accuracy: 0.9376\n",
      "Epoch 14/120\n",
      "375/375 [==============================] - 0s 872us/step - loss: 0.2250 - accuracy: 0.9352 - val_loss: 0.2150 - val_accuracy: 0.9399\n",
      "Epoch 15/120\n",
      "375/375 [==============================] - 0s 877us/step - loss: 0.2179 - accuracy: 0.9378 - val_loss: 0.2093 - val_accuracy: 0.9415\n",
      "Epoch 16/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.2110 - accuracy: 0.9400 - val_loss: 0.2049 - val_accuracy: 0.9432\n",
      "Epoch 17/120\n",
      "375/375 [==============================] - 0s 873us/step - loss: 0.2047 - accuracy: 0.9410 - val_loss: 0.1985 - val_accuracy: 0.9445\n",
      "Epoch 18/120\n",
      "375/375 [==============================] - 0s 917us/step - loss: 0.1988 - accuracy: 0.9427 - val_loss: 0.1941 - val_accuracy: 0.9463\n",
      "Epoch 19/120\n",
      "375/375 [==============================] - 0s 891us/step - loss: 0.1932 - accuracy: 0.9444 - val_loss: 0.1890 - val_accuracy: 0.9476\n",
      "Epoch 20/120\n",
      "375/375 [==============================] - 0s 879us/step - loss: 0.1877 - accuracy: 0.9457 - val_loss: 0.1854 - val_accuracy: 0.9487\n",
      "Epoch 21/120\n",
      "375/375 [==============================] - 0s 885us/step - loss: 0.1828 - accuracy: 0.9469 - val_loss: 0.1821 - val_accuracy: 0.9487\n",
      "Epoch 22/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.1778 - accuracy: 0.9489 - val_loss: 0.1779 - val_accuracy: 0.9517\n",
      "Epoch 23/120\n",
      "375/375 [==============================] - 0s 875us/step - loss: 0.1732 - accuracy: 0.9501 - val_loss: 0.1746 - val_accuracy: 0.9515\n",
      "Epoch 24/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.1686 - accuracy: 0.9516 - val_loss: 0.1721 - val_accuracy: 0.9524\n",
      "Epoch 25/120\n",
      "375/375 [==============================] - 0s 869us/step - loss: 0.1645 - accuracy: 0.9525 - val_loss: 0.1676 - val_accuracy: 0.9539\n",
      "Epoch 26/120\n",
      "375/375 [==============================] - 0s 897us/step - loss: 0.1603 - accuracy: 0.9542 - val_loss: 0.1649 - val_accuracy: 0.9539\n",
      "Epoch 27/120\n",
      "375/375 [==============================] - 0s 875us/step - loss: 0.1566 - accuracy: 0.9551 - val_loss: 0.1619 - val_accuracy: 0.9545\n",
      "Epoch 28/120\n",
      "375/375 [==============================] - 0s 872us/step - loss: 0.1527 - accuracy: 0.9568 - val_loss: 0.1590 - val_accuracy: 0.9562\n",
      "Epoch 29/120\n",
      "375/375 [==============================] - 0s 897us/step - loss: 0.1493 - accuracy: 0.9572 - val_loss: 0.1563 - val_accuracy: 0.9573\n",
      "Epoch 30/120\n",
      "375/375 [==============================] - 0s 874us/step - loss: 0.1459 - accuracy: 0.9581 - val_loss: 0.1541 - val_accuracy: 0.9570\n",
      "Epoch 31/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.1425 - accuracy: 0.9589 - val_loss: 0.1513 - val_accuracy: 0.9583\n",
      "Epoch 32/120\n",
      "375/375 [==============================] - 0s 878us/step - loss: 0.1393 - accuracy: 0.9604 - val_loss: 0.1497 - val_accuracy: 0.9578\n",
      "Epoch 33/120\n",
      "375/375 [==============================] - 0s 876us/step - loss: 0.1363 - accuracy: 0.9614 - val_loss: 0.1471 - val_accuracy: 0.9592\n",
      "Epoch 34/120\n",
      "375/375 [==============================] - 0s 867us/step - loss: 0.1333 - accuracy: 0.9619 - val_loss: 0.1455 - val_accuracy: 0.9593\n",
      "Epoch 35/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.1304 - accuracy: 0.9631 - val_loss: 0.1431 - val_accuracy: 0.9600\n",
      "Epoch 36/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.1277 - accuracy: 0.9635 - val_loss: 0.1418 - val_accuracy: 0.9605\n",
      "Epoch 37/120\n",
      "375/375 [==============================] - 0s 876us/step - loss: 0.1252 - accuracy: 0.9648 - val_loss: 0.1393 - val_accuracy: 0.9613\n",
      "Epoch 38/120\n",
      "375/375 [==============================] - 0s 878us/step - loss: 0.1226 - accuracy: 0.9654 - val_loss: 0.1384 - val_accuracy: 0.9616\n",
      "Epoch 39/120\n",
      "375/375 [==============================] - 0s 870us/step - loss: 0.1203 - accuracy: 0.9657 - val_loss: 0.1362 - val_accuracy: 0.9618\n",
      "Epoch 40/120\n",
      "375/375 [==============================] - 0s 865us/step - loss: 0.1178 - accuracy: 0.9666 - val_loss: 0.1352 - val_accuracy: 0.9612\n",
      "Epoch 41/120\n",
      "375/375 [==============================] - 0s 867us/step - loss: 0.1155 - accuracy: 0.9669 - val_loss: 0.1338 - val_accuracy: 0.9624\n",
      "Epoch 42/120\n",
      "375/375 [==============================] - 0s 866us/step - loss: 0.1134 - accuracy: 0.9674 - val_loss: 0.1319 - val_accuracy: 0.9630\n",
      "Epoch 43/120\n",
      "375/375 [==============================] - 0s 866us/step - loss: 0.1112 - accuracy: 0.9685 - val_loss: 0.1311 - val_accuracy: 0.9628\n",
      "Epoch 44/120\n",
      "375/375 [==============================] - 0s 865us/step - loss: 0.1092 - accuracy: 0.9690 - val_loss: 0.1304 - val_accuracy: 0.9627\n",
      "Epoch 45/120\n",
      "375/375 [==============================] - 0s 877us/step - loss: 0.1070 - accuracy: 0.9698 - val_loss: 0.1276 - val_accuracy: 0.9637\n",
      "Epoch 46/120\n",
      "375/375 [==============================] - 0s 873us/step - loss: 0.1051 - accuracy: 0.9706 - val_loss: 0.1269 - val_accuracy: 0.9633\n",
      "Epoch 47/120\n",
      "375/375 [==============================] - 0s 877us/step - loss: 0.1034 - accuracy: 0.9707 - val_loss: 0.1254 - val_accuracy: 0.9644\n",
      "Epoch 48/120\n",
      "375/375 [==============================] - 0s 866us/step - loss: 0.1014 - accuracy: 0.9715 - val_loss: 0.1248 - val_accuracy: 0.9652\n",
      "Epoch 49/120\n",
      "375/375 [==============================] - 0s 880us/step - loss: 0.0997 - accuracy: 0.9723 - val_loss: 0.1232 - val_accuracy: 0.9655\n",
      "Epoch 50/120\n",
      "375/375 [==============================] - 0s 869us/step - loss: 0.0980 - accuracy: 0.9727 - val_loss: 0.1227 - val_accuracy: 0.9650\n",
      "Epoch 51/120\n",
      "375/375 [==============================] - 0s 867us/step - loss: 0.0963 - accuracy: 0.9728 - val_loss: 0.1213 - val_accuracy: 0.9653\n",
      "Epoch 52/120\n",
      "375/375 [==============================] - 0s 870us/step - loss: 0.0948 - accuracy: 0.9735 - val_loss: 0.1201 - val_accuracy: 0.9659\n",
      "Epoch 53/120\n",
      "375/375 [==============================] - 0s 864us/step - loss: 0.0930 - accuracy: 0.9737 - val_loss: 0.1209 - val_accuracy: 0.9647\n",
      "Epoch 54/120\n",
      "375/375 [==============================] - 0s 864us/step - loss: 0.0917 - accuracy: 0.9742 - val_loss: 0.1184 - val_accuracy: 0.9665\n",
      "Epoch 55/120\n",
      "375/375 [==============================] - 0s 914us/step - loss: 0.0901 - accuracy: 0.9753 - val_loss: 0.1173 - val_accuracy: 0.9673\n",
      "Epoch 56/120\n",
      "375/375 [==============================] - 0s 943us/step - loss: 0.0884 - accuracy: 0.9753 - val_loss: 0.1161 - val_accuracy: 0.9673\n",
      "Epoch 57/120\n",
      "375/375 [==============================] - 0s 870us/step - loss: 0.0872 - accuracy: 0.9756 - val_loss: 0.1164 - val_accuracy: 0.9663\n",
      "Epoch 58/120\n",
      "375/375 [==============================] - 0s 874us/step - loss: 0.0857 - accuracy: 0.9764 - val_loss: 0.1147 - val_accuracy: 0.9682\n",
      "Epoch 59/120\n",
      "375/375 [==============================] - 0s 876us/step - loss: 0.0844 - accuracy: 0.9768 - val_loss: 0.1143 - val_accuracy: 0.9674\n",
      "Epoch 60/120\n",
      "375/375 [==============================] - 0s 868us/step - loss: 0.0830 - accuracy: 0.9772 - val_loss: 0.1143 - val_accuracy: 0.9675\n",
      "Epoch 61/120\n",
      "375/375 [==============================] - 0s 881us/step - loss: 0.0817 - accuracy: 0.9775 - val_loss: 0.1139 - val_accuracy: 0.9675\n",
      "Epoch 62/120\n",
      "375/375 [==============================] - 0s 867us/step - loss: 0.0804 - accuracy: 0.9779 - val_loss: 0.1109 - val_accuracy: 0.9693\n",
      "Epoch 63/120\n",
      "375/375 [==============================] - 0s 868us/step - loss: 0.0791 - accuracy: 0.9782 - val_loss: 0.1118 - val_accuracy: 0.9683\n",
      "Epoch 64/120\n",
      "375/375 [==============================] - 0s 880us/step - loss: 0.0778 - accuracy: 0.9789 - val_loss: 0.1096 - val_accuracy: 0.9691\n",
      "Epoch 65/120\n",
      "375/375 [==============================] - 0s 881us/step - loss: 0.0768 - accuracy: 0.9788 - val_loss: 0.1099 - val_accuracy: 0.9691\n",
      "Epoch 66/120\n",
      "375/375 [==============================] - 0s 863us/step - loss: 0.0757 - accuracy: 0.9795 - val_loss: 0.1087 - val_accuracy: 0.9705\n",
      "Epoch 67/120\n",
      "375/375 [==============================] - 0s 866us/step - loss: 0.0746 - accuracy: 0.9792 - val_loss: 0.1086 - val_accuracy: 0.9690\n",
      "Epoch 68/120\n",
      "375/375 [==============================] - 0s 886us/step - loss: 0.0733 - accuracy: 0.9800 - val_loss: 0.1085 - val_accuracy: 0.9698\n",
      "Epoch 69/120\n",
      "375/375 [==============================] - 0s 873us/step - loss: 0.0723 - accuracy: 0.9800 - val_loss: 0.1081 - val_accuracy: 0.9695\n",
      "Epoch 70/120\n",
      "375/375 [==============================] - 0s 877us/step - loss: 0.0712 - accuracy: 0.9809 - val_loss: 0.1071 - val_accuracy: 0.9699\n",
      "Epoch 71/120\n",
      "375/375 [==============================] - 0s 876us/step - loss: 0.0702 - accuracy: 0.9812 - val_loss: 0.1065 - val_accuracy: 0.9700\n",
      "Epoch 72/120\n",
      "375/375 [==============================] - 0s 878us/step - loss: 0.0691 - accuracy: 0.9815 - val_loss: 0.1056 - val_accuracy: 0.9708\n",
      "Epoch 73/120\n",
      "375/375 [==============================] - 0s 867us/step - loss: 0.0682 - accuracy: 0.9814 - val_loss: 0.1058 - val_accuracy: 0.9697\n",
      "Epoch 74/120\n",
      "375/375 [==============================] - 0s 864us/step - loss: 0.0672 - accuracy: 0.9816 - val_loss: 0.1046 - val_accuracy: 0.9707\n",
      "Epoch 75/120\n",
      "375/375 [==============================] - 0s 867us/step - loss: 0.0663 - accuracy: 0.9824 - val_loss: 0.1039 - val_accuracy: 0.9708\n",
      "Epoch 76/120\n",
      "375/375 [==============================] - 0s 882us/step - loss: 0.0654 - accuracy: 0.9820 - val_loss: 0.1036 - val_accuracy: 0.9712\n",
      "Epoch 77/120\n",
      "375/375 [==============================] - 0s 873us/step - loss: 0.0644 - accuracy: 0.9829 - val_loss: 0.1032 - val_accuracy: 0.9714\n",
      "Epoch 78/120\n",
      "375/375 [==============================] - 0s 876us/step - loss: 0.0634 - accuracy: 0.9829 - val_loss: 0.1035 - val_accuracy: 0.9707\n",
      "Epoch 79/120\n",
      "375/375 [==============================] - 0s 879us/step - loss: 0.0625 - accuracy: 0.9834 - val_loss: 0.1033 - val_accuracy: 0.9707\n",
      "Epoch 80/120\n",
      "375/375 [==============================] - 0s 867us/step - loss: 0.0617 - accuracy: 0.9833 - val_loss: 0.1024 - val_accuracy: 0.9707\n",
      "Epoch 81/120\n",
      "375/375 [==============================] - 0s 876us/step - loss: 0.0609 - accuracy: 0.9838 - val_loss: 0.1017 - val_accuracy: 0.9714\n",
      "Epoch 82/120\n",
      "375/375 [==============================] - 0s 875us/step - loss: 0.0600 - accuracy: 0.9843 - val_loss: 0.1014 - val_accuracy: 0.9714\n",
      "Epoch 83/120\n",
      "375/375 [==============================] - 0s 869us/step - loss: 0.0592 - accuracy: 0.9841 - val_loss: 0.1015 - val_accuracy: 0.9712\n",
      "Epoch 84/120\n",
      "375/375 [==============================] - 0s 873us/step - loss: 0.0583 - accuracy: 0.9846 - val_loss: 0.1002 - val_accuracy: 0.9716\n",
      "Epoch 85/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.0576 - accuracy: 0.9846 - val_loss: 0.1003 - val_accuracy: 0.9716\n",
      "Epoch 86/120\n",
      "375/375 [==============================] - 0s 870us/step - loss: 0.0567 - accuracy: 0.9850 - val_loss: 0.0994 - val_accuracy: 0.9714\n",
      "Epoch 87/120\n",
      "375/375 [==============================] - 0s 881us/step - loss: 0.0560 - accuracy: 0.9855 - val_loss: 0.0999 - val_accuracy: 0.9720\n",
      "Epoch 88/120\n",
      "375/375 [==============================] - 0s 874us/step - loss: 0.0553 - accuracy: 0.9853 - val_loss: 0.0994 - val_accuracy: 0.9725\n",
      "Epoch 89/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.0546 - accuracy: 0.9855 - val_loss: 0.0987 - val_accuracy: 0.9718\n",
      "Epoch 90/120\n",
      "375/375 [==============================] - 0s 884us/step - loss: 0.0538 - accuracy: 0.9860 - val_loss: 0.0993 - val_accuracy: 0.9718\n",
      "Epoch 91/120\n",
      "375/375 [==============================] - 0s 882us/step - loss: 0.0531 - accuracy: 0.9864 - val_loss: 0.0984 - val_accuracy: 0.9718\n",
      "Epoch 92/120\n",
      "375/375 [==============================] - 0s 881us/step - loss: 0.0523 - accuracy: 0.9865 - val_loss: 0.0982 - val_accuracy: 0.9724\n",
      "Epoch 93/120\n",
      "375/375 [==============================] - 0s 918us/step - loss: 0.0518 - accuracy: 0.9866 - val_loss: 0.0977 - val_accuracy: 0.9722\n",
      "Epoch 94/120\n",
      "375/375 [==============================] - 0s 958us/step - loss: 0.0511 - accuracy: 0.9868 - val_loss: 0.0976 - val_accuracy: 0.9726\n",
      "Epoch 95/120\n",
      "375/375 [==============================] - 0s 903us/step - loss: 0.0503 - accuracy: 0.9870 - val_loss: 0.0968 - val_accuracy: 0.9719\n",
      "Epoch 96/120\n",
      "375/375 [==============================] - 0s 889us/step - loss: 0.0497 - accuracy: 0.9871 - val_loss: 0.0968 - val_accuracy: 0.9722\n",
      "Epoch 97/120\n",
      "375/375 [==============================] - 0s 886us/step - loss: 0.0491 - accuracy: 0.9871 - val_loss: 0.0982 - val_accuracy: 0.9718\n",
      "Epoch 98/120\n",
      "375/375 [==============================] - 0s 890us/step - loss: 0.0485 - accuracy: 0.9875 - val_loss: 0.0967 - val_accuracy: 0.9726\n",
      "Epoch 99/120\n",
      "375/375 [==============================] - 0s 909us/step - loss: 0.0479 - accuracy: 0.9879 - val_loss: 0.0966 - val_accuracy: 0.9717\n",
      "Epoch 100/120\n",
      "375/375 [==============================] - 0s 892us/step - loss: 0.0472 - accuracy: 0.9879 - val_loss: 0.0963 - val_accuracy: 0.9729\n",
      "Epoch 101/120\n",
      "375/375 [==============================] - 0s 884us/step - loss: 0.0467 - accuracy: 0.9879 - val_loss: 0.0957 - val_accuracy: 0.9724\n",
      "Epoch 102/120\n",
      "375/375 [==============================] - 0s 903us/step - loss: 0.0460 - accuracy: 0.9884 - val_loss: 0.0953 - val_accuracy: 0.9728\n",
      "Epoch 103/120\n",
      "375/375 [==============================] - 0s 893us/step - loss: 0.0453 - accuracy: 0.9883 - val_loss: 0.0959 - val_accuracy: 0.9728\n",
      "Epoch 104/120\n",
      "375/375 [==============================] - 0s 874us/step - loss: 0.0448 - accuracy: 0.9888 - val_loss: 0.0955 - val_accuracy: 0.9722\n",
      "Epoch 105/120\n",
      "375/375 [==============================] - 0s 885us/step - loss: 0.0444 - accuracy: 0.9886 - val_loss: 0.0949 - val_accuracy: 0.9732\n",
      "Epoch 106/120\n",
      "375/375 [==============================] - 0s 902us/step - loss: 0.0437 - accuracy: 0.9889 - val_loss: 0.0954 - val_accuracy: 0.9721\n",
      "Epoch 107/120\n",
      "375/375 [==============================] - 0s 897us/step - loss: 0.0431 - accuracy: 0.9894 - val_loss: 0.0954 - val_accuracy: 0.9728\n",
      "Epoch 108/120\n",
      "375/375 [==============================] - 0s 899us/step - loss: 0.0427 - accuracy: 0.9893 - val_loss: 0.0953 - val_accuracy: 0.9720\n",
      "Epoch 109/120\n",
      "375/375 [==============================] - 0s 885us/step - loss: 0.0420 - accuracy: 0.9894 - val_loss: 0.0949 - val_accuracy: 0.9730\n",
      "Epoch 110/120\n",
      "375/375 [==============================] - 0s 880us/step - loss: 0.0417 - accuracy: 0.9893 - val_loss: 0.0947 - val_accuracy: 0.9728\n",
      "Epoch 111/120\n",
      "375/375 [==============================] - 0s 896us/step - loss: 0.0411 - accuracy: 0.9897 - val_loss: 0.0950 - val_accuracy: 0.9729\n",
      "Epoch 112/120\n",
      "375/375 [==============================] - 0s 915us/step - loss: 0.0406 - accuracy: 0.9896 - val_loss: 0.0947 - val_accuracy: 0.9724\n",
      "Epoch 113/120\n",
      "375/375 [==============================] - 0s 879us/step - loss: 0.0401 - accuracy: 0.9900 - val_loss: 0.0950 - val_accuracy: 0.9724\n",
      "Epoch 114/120\n",
      "375/375 [==============================] - 0s 916us/step - loss: 0.0396 - accuracy: 0.9902 - val_loss: 0.0943 - val_accuracy: 0.9727\n",
      "Epoch 115/120\n",
      "375/375 [==============================] - 0s 890us/step - loss: 0.0391 - accuracy: 0.9903 - val_loss: 0.0939 - val_accuracy: 0.9734\n",
      "Epoch 116/120\n",
      "375/375 [==============================] - 0s 873us/step - loss: 0.0386 - accuracy: 0.9906 - val_loss: 0.0932 - val_accuracy: 0.9726\n",
      "Epoch 117/120\n",
      "375/375 [==============================] - 0s 871us/step - loss: 0.0382 - accuracy: 0.9907 - val_loss: 0.0934 - val_accuracy: 0.9730\n",
      "Epoch 118/120\n",
      "375/375 [==============================] - 0s 882us/step - loss: 0.0377 - accuracy: 0.9910 - val_loss: 0.0936 - val_accuracy: 0.9732\n",
      "Epoch 119/120\n",
      "375/375 [==============================] - 0s 878us/step - loss: 0.0372 - accuracy: 0.9910 - val_loss: 0.0944 - val_accuracy: 0.9723\n",
      "Epoch 120/120\n",
      "375/375 [==============================] - 0s 884us/step - loss: 0.0368 - accuracy: 0.9911 - val_loss: 0.0939 - val_accuracy: 0.9732\n",
      "313/313 [==============================] - 0s 335us/step - loss: 0.0811 - accuracy: 0.9752\n",
      "Test score: 0.08114946633577347\n",
      "Test accuracy: 0.9751999974250793\n"
     ]
    }
   ],
   "source": [
    "# increasing epochs to 120\n",
    "# \n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training\n",
    "NB_EPOCH = 120\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers\n",
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer=OPTIMIZER,\n",
    "metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAApUAAADdCAYAAADqxYmPAAAMQGlDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkEBoAQSkhN4EASkBpITQAkgvgo2QBAglxkAQsaOLCq5dRMCGrooodkDsiJ1FsPdFEQVlXSzYlTcpoOu+8r35vrnz33/O/OfMuXPv3AFA7RRHJMpG1QHIEeaJY4L96eOTkumkHoACMtAC1kCZw80VMaOiwgEsQ+3fy7ubAJG21+ylWv/s/69Fg8fP5QKAREGcysvl5kB8CAC8iisS5wFAlPJm0/NEUgwr0BLDACFeLMXpclwlxalyvE9mExfDgrgFACUVDkecDoBqO+Tp+dx0qKHaD7GjkCcQAqBGh9gnJ2cqD+IUiK2hjQhiqT4j9Qed9L9ppg5rcjjpw1g+F1lRChDkirI5M/7PdPzvkpMtGfJhCatKhjgkRjpnmLfbWVPDpFgF4j5hakQkxJoQfxDwZPYQo5QMSUi83B414OayYM6ADsSOPE5AGMQGEAcJsyPCFXxqmiCIDTFcIWiBII8dB7EuxIv5uYGxCpvN4qkxCl9ofZqYxVTwFzhimV+pr4eSrHimQv91Bp+t0MdUCzPiEiGmQGyeL0iIgFgVYofcrNgwhc3YwgxWxJCNWBIjjd8c4hi+MNhfro/lp4mDYhT2JTm5Q/PFNmcI2BEKfCAvIy5Enh+shcuRxQ/ngrXzhcz4IR1+7vjwobnw+AGB8rljPXxhfKxC54Mozz9GPhaniLKjFPa4KT87WMqbQuySmx+rGIsn5MEFKdfH00R5UXHyOPHCTE5olDwefAUIBywQAOhAAmsqmAoygaCtr6EP3sl7ggAHiEE64AN7BTM0IlHWI4TXWFAI/oSID3KHx/nLevkgH/Jfh1n51R6kyXrzZSOywFOIc0AYyIb3Etko4bC3BPAEMoJ/eOfAyoXxZsMq7f/3/BD7nWFCJlzBSIY80tWGLImBxABiCDGIaIPr4z64Fx4Or36wOuMM3GNoHt/tCU8JHYTHhBuETsKdKYIi8U9RjgOdUD9IkYvUH3OBW0JNV9wf94bqUBnXwfWBPe4C/TBxX+jZFbIsRdzSrNB/0v7bDH54Ggo7siMZJY8g+5Gtfx6paqvqOqwizfWP+ZHHmjqcb9Zwz8/+WT9knwfbsJ8tscXYQew8dhq7iB3DGgAdO4k1Yq3YcSkeXl1PZKtryFuMLJ4sqCP4h7+hJyvNZK5jrWOv4xd5Xx6/QPqNBqypohliQXpGHp0JdwQ+nS3kOoyiOzs6uwAg3V/kn6830bJ9A9Fp/c4t+AMA75ODg4NHv3OhJwHY7w5f/yPfOWsG3DqUAbhwhCsR58s5XHohwK+EGnzT9IARMIP7lz1wBm7AC/iBQBAKIkEcSAKTYfQZcJ2LwXQwC8wHxaAUrABrQQXYBLaCnWAPOAAawDFwGpwDl0E7uAHuwdXTDV6AfvAOfEYQhIRQERqihxgjFogd4owwEB8kEAlHYpAkJAVJR4SIBJmFLEBKkVVIBbIFqUH2I0eQ08hFpAO5gzxCepHXyCcUQ1VQLdQQtURHowyUiYahcegkNB2dhhaiC9FlaDlaje5G69HT6GX0BtqJvkAHMIApYzqYCWaPMTAWFoklY2mYGJuDlWBlWDVWhzXB53wN68T6sI84EafhdNweruAQPB7n4tPwOfhSvALfidfjLfg1/BHej38jUAkGBDuCJ4FNGE9IJ0wnFBPKCNsJhwln4bvUTXhHJBJ1iFZEd/guJhEziTOJS4kbiHuJp4gdxC7iAIlE0iPZkbxJkSQOKY9UTFpP2k06SbpK6iZ9UFJWMlZyVgpSSlYSKhUplSntUjqhdFXpmdJnsjrZguxJjiTzyDPIy8nbyE3kK+Ru8meKBsWK4k2Jo2RS5lPKKXWUs5T7lDfKysqmyh7K0coC5XnK5cr7lC8oP1L+qKKpYqvCUpmoIlFZprJD5ZTKHZU3VCrVkupHTabmUZdRa6hnqA+pH1Rpqg6qbFWe6lzVStV61auqL9XIahZqTLXJaoVqZWoH1a6o9amT1S3VWeoc9TnqlepH1G+pD2jQNJw0IjVyNJZq7NK4qNGjSdK01AzU5Gku1NyqeUazi4bRzGgsGpe2gLaNdpbWrUXUstJia2VqlWrt0WrT6tfW1HbRTtAu0K7UPq7dqYPpWOqwdbJ1lusc0Lmp82mE4QjmCP6IJSPqRlwd8V53pK6fLl+3RHev7g3dT3p0vUC9LL2Veg16D/RxfVv9aP3p+hv1z+r3jdQa6TWSO7Jk5IGRdw1QA1uDGIOZBlsNWg0GDI0Mgw1FhusNzxj2GekY+RllGq0xOmHUa0wz9jEWGK8xPmn8nK5NZ9Kz6eX0Fnq/iYFJiInEZItJm8lnUyvTeNMi072mD8woZgyzNLM1Zs1m/ebG5uPMZ5nXmt+1IFswLDIs1lmct3hvaWWZaLnIssGyx0rXim1VaFVrdd+aau1rPc262vq6DdGGYZNls8Gm3Ra1dbXNsK20vWKH2rnZCew22HWMIozyGCUcVT3qlr2KPdM+377W/pGDjkO4Q5FDg8PL0eajk0evHH1+9DdHV8dsx22O95w0nUKdipyanF472zpznSudr4+hjgkaM3dM45hXLnYufJeNLrddaa7jXBe5Nrt+dXN3E7vVufW6m7unuFe532JoMaIYSxkXPAge/h5zPY55fPR088zzPOD5l5e9V5bXLq+esVZj+WO3je3yNvXmeG/x7vSh+6T4bPbp9DXx5fhW+z72M/Pj+W33e8a0YWYydzNf+jv6i/0P+79nebJms04FYAHBASUBbYGagfGBFYEPg0yD0oNqg/qDXYNnBp8KIYSEhawMucU2ZHPZNez+UPfQ2aEtYSphsWEVYY/DbcPF4U3j0HGh41aPux9hESGMaIgEkezI1ZEPoqyipkUdjSZGR0VXRj+NcYqZFXM+lhY7JXZX7Ls4/7jlcffireMl8c0JagkTE2oS3icGJK5K7Bw/evzs8ZeT9JMESY3JpOSE5O3JAxMCJ6yd0D3RdWLxxJuTrCYVTLo4WX9y9uTjU9SmcKYcTCGkJKbsSvnCieRUcwZS2alVqf1cFncd9wXPj7eG18v35q/iP0vzTluV1pPunb46vTfDN6Mso0/AElQIXmWGZG7KfJ8VmbUjazA7MXtvjlJOSs4RoaYwS9gy1WhqwdQOkZ2oWNQ5zXPa2mn94jDx9lwkd1JuY54W/JFvlVhLfpE8yvfJr8z/MD1h+sECjQJhQesM2xlLZjwrDCr8bSY+kzuzeZbJrPmzHs1mzt4yB5mTOqd5rtnchXO75wXP2zmfMj9r/u9FjkWrit4uSFzQtNBw4byFXb8E/1JbrFosLr61yGvRpsX4YsHitiVjlqxf8q2EV3Kp1LG0rPTLUu7SS786/Vr+6+CytGVty92Wb1xBXCFccXOl78qdqzRWFa7qWj1udf0a+pqSNW/XTll7scylbNM6yjrJus7y8PLG9ebrV6z/UpFRcaPSv3JvlUHVkqr3G3gbrm7021i3yXBT6aZPmwWbb28J3lJfbVldtpW4NX/r020J287/xvitZrv+9tLtX3cId3TujNnZUuNeU7PLYNfyWrRWUtu7e+Lu9j0Bexrr7Ou27NXZW7oP7JPse74/Zf/NA2EHmg8yDtYdsjhUdZh2uKQeqZ9R39+Q0dDZmNTYcST0SHOTV9Phow5HdxwzOVZ5XPv48hOUEwtPDJ4sPDlwSnSq73T66a7mKc33zow/c70luqXtbNjZC+eCzp05zzx/8oL3hWMXPS8eucS41HDZ7XJ9q2vr4d9dfz/c5tZWf8X9SmO7R3tTx9iOE1d9r56+FnDt3HX29cs3Im503Iy/efvWxFudt3m3e+5k33l1N//u53vz7hPulzxQf1D20OBh9R82f+ztdOs8/ijgUevj2Mf3urhdL57kPvnSvfAp9WnZM+NnNT3OPcd6g3rbn0943v1C9OJzX/GfGn9WvbR+eegvv79a+8f3d78Svxp8vfSN3psdb13eNg9EDTx8l/Pu8/uSD3ofdn5kfDz/KfHTs8/Tv5C+lH+1+dr0Lezb/cGcwUERR8yR/QpgsKJpaQC83gEANQkAGjyfUSbIz3+ygsjPrDIE/hOWnxFlxQ2AOvj/Ht0H/25uAbBvGzx+QX21iQBEUQGI8wDomDHDdeisJjtXSgsRngM2R35NzUkF/6bIz5w/xP1zC6SqLuDn9l+NznxdBtelAAAAAGJlWElmTU0AKgAAAAgAAgESAAMAAAABAAEAAIdpAAQAAAABAAAAJgAAAAAAA5KGAAcAAAASAAAAUKACAAQAAAABAAAClaADAAQAAAABAAAA3QAAAABBU0NJSQAAAFNjcmVlbnNob3Rwp/9gAAACPWlUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNi4wLjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczpleGlmPSJodHRwOi8vbnMuYWRvYmUuY29tL2V4aWYvMS4wLyIKICAgICAgICAgICAgeG1sbnM6dGlmZj0iaHR0cDovL25zLmFkb2JlLmNvbS90aWZmLzEuMC8iPgogICAgICAgICA8ZXhpZjpQaXhlbFlEaW1lbnNpb24+MjIxPC9leGlmOlBpeGVsWURpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6VXNlckNvbW1lbnQ+U2NyZWVuc2hvdDwvZXhpZjpVc2VyQ29tbWVudD4KICAgICAgICAgPGV4aWY6UGl4ZWxYRGltZW5zaW9uPjY2MTwvZXhpZjpQaXhlbFhEaW1lbnNpb24+CiAgICAgICAgIDx0aWZmOk9yaWVudGF0aW9uPjE8L3RpZmY6T3JpZW50YXRpb24+CiAgICAgIDwvcmRmOkRlc2NyaXB0aW9uPgogICA8L3JkZjpSREY+CjwveDp4bXBtZXRhPgru/PaKAABAAElEQVR4Ae29d3Qcx5X2fScPciKRCJJgziLFKEqiRCUqB8qyLMnSkWzL9q4c9vv+eL1rb/T67Hr3/Xw2eb3He7TKOWuVKCraFCVKFCmKIinmBCaABEAQGZPw3afABgcgSAQOBt2Dp6Th9HT3VFf9unHnqVtVt1ztmoSJBEiABEiABEiABEiABM6BgPscvsuvkgAJkAAJkAAJkAAJkIAhQFHJB4EESIAESIAESIAESOCcCVBUnjNCZkACJEACJEACJEACJEBRyWeABEiABEiABEiABEjgnAlQVJ4zQmZAAiRAAiRAAiRAAiRAUclngARIgARIgARIgARI4JwJUFSeM0JmQAIkQAIkQAIkQAIkQFHJZ4AESIAESIAESIAESOCcCVBUnjNCZkACJEACJEACJEACJODtCYHL5eppN/eRAAmQAAmQAAmQAAmQQCeB+IUZexSVODP+pM5vcoMESIAEhikBNLZpF4fpzWe1SYAEeiTQ3QnJ7u8eMXEnCZAACZAACZAACZBAfwhQVPaHFs8lARIgARIgARIgARLokQBFZY9YuJMESIAESIAESIAESKA/BM44prJ7Jt37zbsf52cSIAESSCUCfRk/SbuYSnecdSEBEuiNQG92sc+iEhf6/PPPe7sej5MACZCA4wksWLCgz3WgXewzKp5IAiTgYAJ9sYvs/nbwDWbRSYAESIAESIAESMAuBCgq7XInWA4SIAESIAESIAEScDABikoH3zwnFb2pqUlWrVol0Wi0x2Jv2LBB9u3b1+Mx7iQBEiCB4UygpaVF3nzzTWltbe0RQyQSkffee09isVjn8d6+03kiN0gggQT6NaYygddlVg4jUFNTYwI/nzhxQjIyMiQ3N1cOHTok+fn5ZhvVOX78uOC8goICycvLMzXE+ceOHTPbq1evlsWLF5t8jhw5IjB6JSUlkpmZKRCV48ePl/LycnMu/mlra5OqqirzXlRUJFlZWeZYY2OjHD16VLxer5SWlpp3XBcvlAfXPnz4sDmGiRQ4t7CwUOrq6ozRbWhokOLiYnN9lDktLc189ng85nhlZaXgGrgmjHh2drY5JxQKSW1trdnPCRqdt4kbJEACZyGAiQ2wKT6fz9gg2E7YvJ7sZ3V1tTkGuwgbAxuJ85DeeustueiiiyQQCBg7BFuEvEaMGGEa6xCVl112WWdJYLvivwObC7uG82EjrXJhn5UPtmFzYQthW4PBYGd+3CCBvhCgqOwLJZ4jb7zxhjGM5513nvE4Tps2zYjHzz77TH7+858bQ/T000/L9OnTZevWrXLLLbfIqFGj5F/+5V9k/vz5cuDAAYHBREv6tddeM8YNAvDFF1+UP/uzP+uRMMQo8sJ3XnrpJfnxj39sjOl///d/y5w5c4y4hXH++uuv5eOPP5ZZs2YZg4z3f/7nf5Z/+Id/MIb8kUceMddAHTZv3ixXXnmlEajYDofDpmwQtBdffLE8//zzppyTJk0SiE/8GOD99ttvl/Xr18uePXvkrrvu6rG83EkCJEAC3QnAfj388MPGNsF+/td//ZexX+PGjZPHHntMfvaznxnh+PrrrwsmQuzcuVPGjBljBOSDDz4osEUQe2goI/3xj380dgx2DkJyxowZsmjRou6X7fL5nXfekYqKCpkyZYrgOjfccINp7G/atElmz54t9fX1RkiiPLDXbrdb0tPTTWO7S0b8QAK9EGD3dy+AePgUAbSSr7nmGiPe4Om79tprjeFByxYzYOGFhJi84IILBEbsq6++MgbvxhtvlOuuu84YVXgu16xZY7aRMz5v37791EXitnBs//79xtOI1jq6x99//31ZsmSJ3HbbbSZPtNpfeeUVueOOO+T666+XSy+91BhEZNNT6IOFCxfKsmXLjKg8ePCg4AWDDXEMAYn3+++/3+Q9b948kx8ML66NY5dcconxIMQVk5skQAIkcFYC8DrCLi5dutQ0tkePHm1EI3pf0JPyxRdfGPsJ+4UGLITj3r17TZ6wnZdffrnxMEKgvvvuu2b/7t27Te/QBx98cMZhRTgR30EvEfKF/UI50ECGNxTeTtg3eCTRA5WTkyPr1q0z+9FgZyKB/hKgqOwvsWF8PgwPjKPf7zdiEijQTYLxPPA6QvihxYt3iE7sQ5cLPH3wVDY3N5uum5EjR5rWN0Tb97//fdN67gkrDOvUqVPlpptuMiIU10GX9K5du0w3Eq6F8iA/7MO4TQhRlA8JnlGUBfuQcC66svEOMYkud+QN7yq6itAyx3chZJEXyg1Di3PgHUV9ysrKTF78hwRIgAT6QwC2ErYH7+gKtxJEH4bnQETC5qCRDcGHLmrYIfTYwH5i+A6+DxsELyd6XO688075wQ9+YPK08uv+ju/Abu7YscPkj+vgM+zZn/7pn8oVV1whv//97801br75ZrnnnnvMNdGj1FPDvHv+/EwC8QQoKuNpcPuMBCASrTGNEHEYgwNjhRY3xiTCi4l3dDVDnKF7BV0t5TpG8vHHHzcCD13mEGkwhBB0r776qmkVw3DBiCLP+ARjB28nzpswYYIRltiHPJ966in53//9X8E4xwceeMC09nFteEzhvbz77rtl5cqVppsI3Ucw5DCk1lhPdC/Ba/nss88aY42ywsj+9Kc/lU8++cR0S8EII6HLCgkeUtSZiQRIgAT6QwD2xrKfEIVnsp/oJt+4caMZYoMxjRCOzzzzjOkSnzt3rhGj3/72t81YS3Rjo+FtNa4xhCc+Ycw5bB8ELIbsIF/kD/sMe43eFwwrgucTvUmwfx9++KG88MILprcH4zNp7+KJcrsvBFz6g97e/UQ8SN13Y9+5BvlFqwsPLca//fKXv+z0KGFGMLo14VHCda666irjhu9eLn4mgWQTgEcTYzHh4fzWt751Vo9AssvG6w0eAYxt68kG9rSvv3YRdhBDQNB4+Yu/+ItOO4i8LTsImwhPPhpTTCRAAiRgBwJ9sYtJnaiDFhLGjEBYwuVvJbj3P/roI/mbv/kbWbt2rfFewSOF1N2IW9/hOwkkgwA8qN/97nfNDPOeGlvJKAOv0TMB3A8nJthBeIHgOYq3g7B1GLIBrxaiEqBBY4lKnIfjTq2zE+8Ty0wCTiVg2QpMuEp2SqqohEFEJbsbRnRhYiwbuijRdQl3PhLAoOWOiRQ4lgoJdUWyxv05vU64R7hfGOCdKgl1wvhPdNWnSsLfUCoNvMekKgyjcGKdYP+620A8Z4hEADGJqAmwd3gGrR8HRCrAcA4M4UilBNuBXoBUSlYsyVQLx4N6oXGDRlEqpVR8BjEBC5O7rKFbybxfSRWViDsIjyQ8kytWrDCtdYRJmDhxommdv/zyy2ZgMrq/rYRJE7/+9a87x6NY+536jrAOSPBGpEJCN93bb79tZl6nQn1QB9QJXZqYJZkqCbM/ETIpVdKvfvWrs854tXM9YQcRSQC2wLKDmFCGMXEQyRgGhHPiBTMEJqIbYDxdKqVUey5xb1LNxlvPG+qF5xCTJ1MppeIziFBTCE01FCmpohLeOahnK0ArBhDDcKJVjhlnmN0LT2Z3ryQ+d983FLAScU10pyKlSn1wvy688MKUqQ/uDeo0c+bMlKoTYtGlyjNnee9wr5yYYAcRfsuKLRhvByEaYQeRsD/eo4nnEq/4fU6sf3yZU+m5tOqVajY+vl5ocKeKHbHqlYrP4FDaiKSKSlS0e7cvjCRST8esmx7/jpALVkDs+P1O20Z94a20DJDTym+VF/XACg2plFCnVOrOx73pPrM+le6X0+qC5wuCMT711w6iGxKREayu1vi8nLiN3wUsaGBxcGIdrDKnWrd3qteLttG6w4l5T6qoTESRMZEHgVthgJycMMYNLupf/OIXKeV5cPI9YdlJwCkEMIMcExsxezwVhNhDDz0k//RP/5Qyw5yc8hyxnCSQaAKOE5Vwv0NQYlUUJycMpMXgeyYSGEoCOidJJ4N0vE52GpjP8WXCcXWuSQzBx/DS1NwS0zGNIjnZbh2833Ec++PPqa2LydFjUcnPc5s8W1raVTS4JRyKic/vNnm4NL+IfunAYZ35XB+TIj23ZGTHseLCjsl5tcfbZe+hqDS1tUu6XwsSbTfXxPWGc8IEF3ShO707EsMZsAIXEwmQgPMJOE5UJhs5Zoahu90a/9Tb9eFJxQzNsWPH9nYqjyeJAH60kNDtiNTxuV21CcK0qDjCuwob/Od162oXHp8RR+FoSAJev37PrcInIq3hFmkLt0k4FhaNYyA56Tniceks3VCTePR7bt0ORdqkLRISr8ct9aFGaWxrktKsInPtmIoh/Zp+V4WUCrI2DQTQEmrTAbZtEtP/whGoMwimqBFNzU0enWmpY+tcWdLuPaFizCu1x6PibveLL6Ne2lo1n5Ys8bvTJebWawabJRb2SE6WT3YdbJRte9rF73FJekaLZKS1SzBN9RgEXGVECrJdUteoQu64S4LRHL12RII5DbpcW0iOVeswlUBE0rK0Lk1eCbfies0aaD4qoeZ0cXsiEgZTLS4myEdavBLMapGIKJumTGWgHD0x8fuCUlQQk6rjEeUZkdwMvxysqRe3Nyx+V67k6Oemtno9V2RcmV/SskOy5qBLGr7WrmGfriRS45YMf4a4/Q2SX9wqaUG3TuSLyLxJY5SnXpwpqQT6awsxIRO2E0sDMpEACQwPAhSVvdxnjFv693//d7PSSnl5uZmdjtAf8BJgBRkseYXxUZgRh1ALWLEF7zgHSwdCwKCrCsetFRV6uaQtD0fVHQUxZHW1oV4xfXnUvYXtiAotyKWoCiJ8bg43GqECseR3Q5i5JKQirTnSpIIpptpKRVe4QepaThhPS4c4a1Zh5lZvVIaeryFWomEj5GIqhGIq6ny+gJnx264CJRKNSEtbi/i9PmlRsYcU0f0oYxTX1+tE2lUE6XkRLRM8bK0q4lBMiMhQqF1yc3TSg9slx2t0O1OXdoxpmVSptal48qlHLNTq1nBWIqML0+RYfYs01wcky58pQb+GvWrU63sqJar/BWP5KnzatDwRyQ5kSSwU1LKHpK0xS+qb/VJU+keNc+kXd8wj7XDNqYhChKyAt10CnqDUHQ8qN5Ep471aT5cUjtDztA7ijkrkeEwqKjVkU1qOZKuXLyPdJfVNrVKWkSfuNLcca9otNXXNkq6huEZ606VV1er6zWEpKUqT+Ysgk0Uyg1nmPuEe4DV5kte8g1ko2qZl3GOEdGYwWzK8I7RGKii9QckJ5ijbZr2XzZLpH2dYN7Y1iM/tUyGowlvvC1Kb5pEVUC5alwa9px69h3gWGtsatY5BzdtrvoPzMnyZJp/q5mPmXual5Zs88NwE/Wn6zIRUtIdUTGbqdlia9D7mBHP1M8KYqJRX4Q62n7s+Nd/jP8kjYNnCH/3oR2YlLdg487evDZJytY1YgQqTjBDNA6uzIDwcxlvj7x3iEmPXsNwgzkXD22rkJa8GvBIJkMBgE0gZUYnf4IEkF1TIWRK62/GCAd2zZ48Zw/Tzn/9csOQgAhXX1NQIurLffPNNwf7nn39e5s+fb1bDuO++++Sv/uqvjCHFklp/+7d/e85dVUZsGKnQIY7gMVPJZYy7EUuRVvW2+cwZ+PFtCbVITVO1iqXjUqdeocKMQiMUWtWjFvAGpLa1rkN86eeY1hOeO5+qHpUfmmuHNwj7WlWgAZVXf9QjKjs6eOs6trpP/WoqNjseJYhMpFyIB0+Hlw9iEqrOp+LS507TcztWbHKrdy9bBUNYBSMEyoj0IhVXsQ6RqPfTJQFJc6kXTT18oWavbN7bLMfqtPu03iujSgKSn50mlRBYI9LlWK1IQ5OKWjj+1DuHHzRPVAWq5uJVV1ik3SUF/pikBVQ7qnAL6L66Y1pH1ZsTVKy1NmnsUK0M5k9401WMhtySVuCWzPEhqaoOyZyigIxfFJSmRmWh1xgxQqMUuOHtVNEa8klBLsReu3rm3OqpExV6MSnRczIz1CN4ol29bCqSdb+unCY+ffkDWi4tJxK8h4pYj/XyMJqzu/0zudtnfFzQw74k7RohI/t0pZy0rktynulLPeXndulNZDojgYHYwt7sIC5m2UIIR4RA+vM//3P52c9+ZuJqwg4idiji8D7xxBMmDBwa3Fii9R//8R+NHcSwJawShHWmf/vb3zq6kX1G+DxAAsOcQEqIShjRj78Iy+dfdYTi6Os9XX6VT8rLzo4A67SiK/vyyy+XLVu2mGCiWNca6V//9V+NYESLGy1xxJeLT1h39brrrjNrS2NCTktLS5fYcxBrEH4qCc3XIBghAA/VHTTeoV21u40nLUs9dxk+FU7Nx+WEikC3ihD1sWlPqYoQvTbywXtY5Z1fvXnw0CFBxGXrd8fllktuMF/KcsfIsYZjMlLFW9Af1K7cVplUMNmIUHz2qjCElwndvcarpVeJQUxq3vAkQhjqpdVbhNxVkOl2BF22eh72HVfxtG1HRObMQjdtx5i76tqY7NgbkzElyEvkozXq3fDp7Fc9P6D6EzXHfjjwNEv15Olnfen/Au0QVPEV1JPztJv1ihkeKS32SFamW/ZVRIywVWexHKmKyfkT3VJc7FXBCtGoX9aEblUkPB+qX00ZtSqdKf7HN35/5wlmozcB03UWr/XdksIOcY3PRYVxF7VOiHv3AN6p0+OOcJME+kcgpJ74Z95sFXXy9jmpI1nuuiGotuPsX7FsIdajhmBEKJabbrrJNJSxPvW2bdtM1AS8799f0SUzrEMNu3n++eebIO5YLQgNcyYSIIHUInB2ReWQukIQLDzPJ3On9686AQz67yVhEDzCdqALGy11KyQSPJdYu/d3v/udVFVVmZmYHV7EUxlaYUOsbp7ux7dWbZUXNj6nnj8oKvWwqXDLTsuSMdmjTXfieSWzzX54+qyuwyzTldmhliD+IMvgrbSEHq5lXce6rsnk5D9j8sqNyGprixnPWFOzdsHqj0ldnXoDw+3SoD3JYdWkzeqRi4RjulqOrphjztFxdKrZ69VLB0GYo72R1SfQtd2RMTybXuVZWuyW515rlZHahYt883Ts3qUXeKW6BuMWRf7fP02D/jWCz+frmMCBHLDPiL84cYV91rGOrVP/Tp546l6PKTu1vz9bVv79+Q7PJQE7E4AX/O4b1S3ez2RN0jrb12ALEfwajWd4K2ELLRvz6aefyl133aUNu2IT0N00dLtlBmGJhCE06BJnIgESSD0Cp36ZHV43tLL9UDsJTqWlpWa98scee8y0si+66CJjSCEYf/CDH8gLL7wgOAceScQnQ0scLXpsL1261JyLrtglS5acFptuevEMuWvuPZ2GuT9FhwBTjdvh0tM3TPyAdxDG3K3dqpgUgn1VVVHZtDOqwljHP+rdhjNCtaN4VNDpfAodK6djDdWrMSJLJ3SkqVdQxaI6LXXMKLp3RSaOc6ln0GUmlcDzh9m+LTrzt0HHGo4oUA/myS7c+LJfdpFm2i0NVPh1y4YfSYAEzkIADSWM1x2MBDuHhStgC7E4ABY9sNL9998vf/jDH6SgoEBuu+02M6YSjVusSgUxCu8mjkGE4nsYe85EAiSQegRSRlQO1q1Ba/yOO+7ozB4rYSDBOF599dXm1XlQNyAurXTvvfeaTawhfffdd1u7+/QO0YgXfiQwiQMN+4oDEdm7X72HTTE5cFTHHmp3s09FHbwMOu9ECjQUC4QgPI0+/WFR3ShjS91y/RUBKdBQLUjIBw5OnGflb72frWBZcQfT03WsIuZNMJEACQwbAt1tYXzFZ82aJXh1T9/85jfNrh/+8Iedh771rW91bnODBEggtQhQVA7x/UT3c3VNVJq1W7m+ISYntBv6qMbla9WuZ8xUdmv/slvj+kE4jtTxhOUqEiemeeUqnRCSoeIOCaIQxyFA8TpbwnlWss613q39fCcBEiABEiABEiCB/hKgqOwvsQSev2FTRP7r0VYZX+aWgIaKyct3S/kYXUt7YUfoGHhDW1tj2pWOiTMJvDCzIgESIAESIAESIIEEE6CoTDDQ/mQ3fbJH/p/vp+mA9zMrxjSNRchEAiRAAiRAAiRAAnYnQMXSyx1CGKDdu3f3ctapw5gV+fXXX5vwQaf29rwV0HA5iIvIRAIkMHwIwKYgcsSDDz5o4t/G1xyRJN577z1544035MiRI/GHhnwbK+p89tln/SrHnj17+mU/+5U5TyYBErAdAYrKXm4JVpFA0PINGzaYUBp1dXWCfVgZArMbERoDgYC//PJLQew1/BBgxQn8MOzfv9+c08sleJgESGAYEcCElxkzZsjx48dNmDKr6ghT9uyzz5ooEViVBivQ2CnB7v3mN78xs7xh8xDwfM2aNbJx40YT9By28PDhw0YwYx/E84svviiPPPKIrFq1yoRms1N9WBYSIIHEE0gZUdmxcnP//+0NKWJUIjYbfgAQ/Pyv//qvpaKiQh566CH54IMPBGt9/+d//qdZTQLLMsKQ4jsQn/geEwmQAAnEE0CIHSzZivf4BNsBsQbRiViQr732WpdGKUQbYuVacWjjvxu/3X8rqDP9+pDiV9RBg/nv/u7vzKpiltcVQc///u//3qwwhkY37KD1HWtJxz5chqeQAAkMkABsA/7mhjIObEqMqYQR/WT/J7L+0Pp+3Yqbpt0s5Xljz/odaxWJyy+/3BjRY8eOGU8ljP6KFSvkgQceMMZz9erVcv3115vgv1jvFvEsx48ff9a8eZAESGD4EcAQGYgyLO8KITZlyhRdZKDexHZEWB4IMAhLK7C4RQg9IRhacza7ghW6nsWCClh/tI8JS6veOfsODUN2enzZ+CwsW4iYk4hV2djYaJaubWtrk3379pnVdbCKGDyaI0eONEHOsRoZfuiuuOKK+Ky4TQIkMAgE8LeI1a5gX4YqpYSoxGouC8sWyPklc/vFEWtf95bgTbBW1Jk+fbox+Ii9BkOZpmsEZmdnm/VvYWB/+ctfmhdWjIDoxDq4CJLe/ceht2vyOAmQQOoSgO2AcLz11luNRwFeBdgK2A3YFvwoYBvrZMfbjjFjxphYkNjX0NDQIyCv2yd3zfl2j8fOttPdh/ASsIXWijoTJkwwgviWW24xHleUPycnxwQ+R1xeeCxHjRolWPgBQ4KwIhnsJerFRAIkMDgEsOgKlj9FT8GOHTsG5yK95JoSohJ1RCsbAb8TnUpKSsxN+vWvfy133nmnMaBopWdmZpq1bCEmX375ZSM8b775ZoHh/5M/+RN5+umnBYHSly9fnugiMT8SIAEHE0BDc+rUqV1qACGGBOF2LmtiQ3B6+iAQu1y8jx/iV9TBCmHwWKK3BkIRHlaIxg8//NAISKyac8kll5jtl156yXg2sQAEGuFMJEACqUsgZUTlYN0itLR/+tOfdmaPAfbf+MY3Oj9j4y//8i+7fL722msFLyYSIAESSBUC6JKPX10M9ererX3fffd1qS4a3xgixEQCJDA8CLAvYnjcZ9aSBEiABEiABEiABAaVAEXloOJl5iRAAiSQGAI69JKJBEiABGxNIOnd39Z0d4zDwcsaiG6Fy+i+/zR6uvhMW2ubGatz2jEH7WC4IQfdLBaVBGxAQCOFdEmYRY4JO91DE3U5yQEfMHEJE32YSIAEnE8gqaISMwCfe+45IyYx9f073/mOCZ0Bg/Lkk0+K1+sV7MeYxfz8/B7p+kZ45YlHHzfByHs84eROtOo/XhuRSeM9Ulx45mUQz5bHYB6DmEaYIktUD+a1mDcJkIDzCUSip1yVmOV5ww03yAsvvOD8imkNJk+eLKgTEwmQgLMJJFVUIiZbcXGxmcTy5ptvminvM2fONCszIK7SL37xCxPwd926dbJs2bLTyLaGW+Xrli3y77/+reSlnXm1ieMn2uXlN1vl/yz1yhVLfCpiT8vKFjsoKG1xG1gIEnAEgXhPJWaQYzY1EwmQAAnYiUBSReWZKl5YWGiE5vvvvy9Y+7agoKDzVHSLY8mvouIi2RvdI2Nzx0p+el7n8fgN7UWRY9UxeeaVVrnhKr+ML/eqJzD+DG6TAAk4mQC6e7FyVWVlpZOrMaCyx4vKAWXAL5EACZDAIBNIqg8PsdnwY/Doo4+a5ciw2gK8kkhY1rC6ulry8vJk0aJFndXGGMvZs2fL6PLRsmbfR7Kg7NSxzpNObuzZH5Hf/L5ZrldBOWEcBWV3PvxMAk4ngOUN58yZY3o8nF6X/pZfh1Dqogv9/RbPJwESIIHkEUiqpxIBfu+9916zioQ1Ief88883YywRSBcDtjHoHMfiE5ZhXFvxqUwvniWFWYXxhzq3q2tj8txrIfnFT9MlJ7vr9ztP4gYJkAAJOJRAVEUlEwmQAAnYmUDS1RdEI8YD4R1jCq13TNLB/u6CEvDQBf7FgfVyzdTrepzYoiueycPPtMrdt/olNwczyu2MnGUjARIggf4TiMbopuw/NX6DBEggmQSSLioHUrlYe1TcLrek+zqWMovPA7O8X327VRae75XRo5LqeI0vBrdJgARIYFAJRCIUlYMKmJmTAAmcMwFHiMqmUJN44MXs5oLE+KI/rglJc3O7XHKBnx7Kc34cmAEJkIBdCcRFFLJrEVkuEiCBYU7AEaKy8kSljMkuO63ru/JYTFZ+GJZv3BCwbdigYf58sfokQALdCGDsOBaBCOu4HWx3T9jX0zF6KruT4mcSIAG7EXCEqDzccEjKskd3YQdbvPrTkFx1qU8y0h1RjS7l5wcSIIHhSaCxsdHE4/3JT35iFnuIpwBBuWXLFrn//vtPO4aJOj1o0Pivc5sESIAEhpSAI9TYvhP7pDi7tAuomuMxOVgV025vH7u9u5DhBxIgATsTyMzMlJtuukkmTZp0WjG3bt0qO3fulBkzZpx2LNJtmcbTTuAOEiABEhhiArYXlbH2mDQ010teRteA56t0LOWF8zBjnFO9h/gZ4uVJgAT6QQBRL3paTevEiRPywQcfSGlpqRw/flwOHDjQpXt8377tsnr1ahPTtx+X46kkQALDhEBzc7OsXbtWNm3aNGQ1doSoDLg0BJHL0wmppSUmew7GZM4MX+c+bpAACZCAEwi0tbUZcVhRUSErVqwwIhFd3girdsUVV0haWlqP8XpLSqfIxRdfLLm5Z16i1gn1ZxlJgAQGh0B6erosXLhQZs2aNTgX6EOuto/BgzFGaNW740Tlrv0xGVPslkCAXso+3GOeQgIkYCMCfr9fLrzwQlm8eLEpFcQkVgpCjN5p06YZ7yS6v/E53qPJ4Oc2uoksCgmQQI8E7C8qdTWdmP4Xn7Zuj8j0qbYvenyRuU0CJEAChgCEIoRkfMIiEFbC8fjP1v5oFGuLMZEACZCAfQnYvvtb5ztKREWl+6RTEoZ1f2VMxpU5oOj2ve8sGQmQgMMIhLFMI1Wlw+4ai0sCw4uA7ZWZCaGhq+mIdKjK6tp2yQyIpDOM0PB6UllbEhjmBDS0JRMJkAAJ2JqA7UUl6KUF0jrHFlUciUrZKKwbbmuuLBwJkAAJJJQAQwolFCczIwESGAQCtheVGF+U48/urHpFRVTGjT41/qjzADdIgARIIIUJRND9zUQCJEACNiZgf1Gp3d65wY4QGugKR8DzUTrzm4kESIAEhhMBLNPIIZXD6Y6zriTgPAK2V2fwVGb5Mw3ZhsaYhHVcUW6O7YvtvCeBJSYBErA1AXZ/2/r2sHAkQAJKwPbqTNeekEx/lrlZhw7HpCgHq1Hw3pEACZDA8CJgRCVdlcPrprO2JOAwArYXlZj0nRHI0IDAIvs0lFD+SPsX2WHPAItLAiTgAAKc/e2Am8QiksAwJ2B7hQZPZbov04jKgxURKSnkJJ1h/syy+iQwLAlEwnRTDssbz0qTgIMIOEJUZgWyJKqL6lRVtUtJAfu+HfR8sagkQAIJIhDWhR+YSIAESMDOBOwvKnUAZdAflJiKyprGdsnIpKi08wPFspEACQwOAXR/U1YODlvmSgIkkBgCtheVqKZbV9RpbY1J1OOSjHSKysTceuZCAiTgJAKIfMFEAiRAAnYm4E124aLa3MbL4/GI2+02K+W06ywcvCIa3Rf7cAyhhOJTdU1MSnNdeqzr/vhzuE0CJEACdifQ0tIiGzZskC1btsh9990nPp/PFBl2EfsrKyvF7/fL4sWLJSurI/IFToCnkq5Kg4r/kAAJ2JRAUj2Vx48flyeffFJWrlwpDz/8sDQ3Nxss4XBY/uM//kNWrFghjz/+uBw+fPg0XMeqY1JemtTinlYG7iABEiCBcyUAwThr1iypq6szDWwrPzSkp02bJtddd50Eg0EjMK1jeFczyUQCJEACtiaQVJW2Y8cOKSkpkRtvvFHKy8sFn5FgTDMyMrSLu1VCoZDxVMZTQwv+YGVEyscktbjxReA2CZCADQigRwP2IIZB1g5N6ImBvUOvTHzC5/T0dNm9e7ds3rxZFixYEH9YwrGo9uZETa9OlwP8QAIkQAJKwOrxhY0cqtTVqg1yKdBCh2hExeGlDAQC5oo1NTWyb98+WbZsmYwePVr279/fWRL8eKxbt04+27hNRtFT2cmFGyQwHAnU19fL2rVre+zNcAoPDPPZu3ev1NbWyrZt2wTd4VVVVeqJDMsrr7xiXhdddJE0NTV1qVJD/RHZunXLafu7nMQPJEACw5ZAW1ub7Ny5UyoqKoaMQVJF5dSpUzsNJ8Tl2LFjTYs8NzdXZs6cKatWrZKGhgYjLC0iaL0vWLBIRhRMlfx8xqi0uPCdBIYjgZycHDPWsKyszLHVR6MaAvLWW281Hlc0nK3GNrq/r7rqKrMf58Qnb1qZzJx1nmRmdixbG3+M2yRAAiSAYTOwIePGjRsyGEmdqJOWlibLly/vUlmISaQ777yzy/74D2qDxaPeXK+bk3TiuXCbBEjAeQQwMQcN7PiE7nAk/CCcKQ1hj9aZisT9JEACJNCFQFI9lV2u3I8PEJUoaLchSP3IgaeSAAmQgLMJRJw7jNTZ4Fl6EiCBPhNwjKhE0F+dz8NEAiRAAsOSQIyRz4flfWelScBJBJwhKpUoGukUlU56tFhWEiCBRBKApsR4TCYSIAESsCsBZ4hKtaOYIE9RadfHiOUiARIYbALQkxxXOdiUmT8JkMC5EHCMqHT5KSrP5UbzuyRAAs4n4ODwnM6HzxqQAAn0SsARohLrfgcCHFDZ693kCSRAAilNgKIypW8vK0cCjifgCFHZ0touaX4Xu78d/7ixAiRAAudCQBfUYSIBEiAB2xJwhqhsaZf0pEbUtO39YsFIgASGKQH01dBTOUxvPqtNAg4h4AhR2dwikpnuEKIsJgmQAAkMAgGs/aDLfzORAAmQgG0JOEJUtrTEJCPdEUW17Y1mwUiABJxNwKMmMMpglc6+iSw9CaQ4AUcotRb1VGakpfidYPVIgARI4CwEfBCVEcapPAsiHiIBEhhiAo4QlU0YU0lROcSPCi9PAiQwlAQ8Hu3+pqYcylvAa5MACfRCIGGi8sCBA9LU1NTL5QZ2GKIyLZiwog6sEPwWCZAACSSAAFbFiWoU83A4fNoKOTiG/ZFI5LRjXh1T2c71vxNwB5gFCZDAYBFI2JzqEydOyKeffmoMYklJicyfP18yMzM1DNC5x5ds1O7vNE7UGaxngPmSAAkkkUBjY6O89957snLlSvm3f/s3CQaD5uoQlB988IHs2rVLZ3nH5IorrpDJkyd3lgyeykiUrspOINwgARKwHYGEicopU6aI2+2Wjz/+WD7//HP5+uuvZfHixTJ37txzrnRTWEVlGj2V5wySGZAACQw5ATS2b7rpJtmzZ0+XsjQ3N8vmzZvlgQcekBYdSP7II4/IxIkTjV3FiT50f9NT2YUZP5AACdiLQMJE5apVq3TVm4DcfvvtkpWVZbp3duzYcc61xXq3reGYdn+fc1bMgARIgASGnAB6bwbSg9PWfFAb65uktHiy6QUa8oqwACRAArYi0NraKnv37jWvoSpYwtx/EJQLFy6U7Oxs462E0ZwxY0ZC6hVp1VY6pj4ykQAJkIDDCcDwv/nmm4JG91NPPSXV1dWmd8fv98vMmTPloYcekieffFKuu+66Ti8lqpyTWyLjxk2XjIwMhxNg8UmABAaDAHTYpEmTZMyYMYORfZ/yTJinct++fTJv3jxjBKGUFy1a1KcC9HYSPJUu7fJJwNDM3i7F4yRAAiQw6ARg+K+99lq55pprzLW8Xq/k5eUZ23n55ZfLJZdcYjyZHgyijEs+9H+73APycsZlw00SIIEUJQBnHuxJd9uRzOomTFRi/ORLL71kyo5xQD6fL2H1gGmlqEwYTmZEAiQwhAQswx9fhPgfgTPZTrcuqcPZ3/HUuE0CJGA3AgkTlUVFRTJ16lTBYHPMXMRMxoGMG+oOCJ5KIyq7H+BnEiABEhhGBLwel45VH0YVZlVJgAQcRyBhovKtt94ycSoRYy0UCskFF1zQZTzQQMmY7m+NSkRP5UAJ8nskQALJItDW1mYa1Xg/cuSITJgwQTBWMhFJe7UkwtnfiUDJPEiABAaJQMJEJbyTmKizbds2M/Acn5lIgARIYDgR+Oqrr8zQH9hBJAhLjJNMRPJql02MnspEoGQeJEACg0QgIaISXd2zZ8+WsWPHCoKgY+ZRT+OC4MVcu3atEZ0FBQUmjiXGEsGz+e6775owRMeOHZNly5bJ6NGju1b53GOod82Pn0iABEggwQRg97788ksTVu3SSy8VhFpLVPLrMPUIRWWicDIfEiCBQSCQEFGJcqFlnp+fb1bSOVM5t2/fLlVVVSbw74svvmhiKWFSD7qHrr/+esFKE48++qiZCWnlYa0fQU1pEeE7CZCAXQlgEYjCwkIznhwhgy677LKEFTUjwyXNumQtEwmQAAnYlUBCRCUm5CD22hNPPGGMKaa0/+hHPzJT2+MrXl9fLzk5OWY/JvbU1dXFH5b169ebZcmw4kR8isQixosZP0My/ji3SYAEhgcB9Ipg3Wy7Dq/ZuHGjWQQCjez09HQzzvyqq65KyM3JpKhMCEdmQgIkMHgEEhZRfNasWaY7GxN0MLYSSzZ2T+Xl5XLw4EGzFBla8eguh+cSPxRNTU2yZcsW893477VrDI2qyh1SUbHfnBd/jNskQALDiwDsBJaARcBwOyY0iNetW2fW877wwgtN70uiyqkaVZpa6alMFE/mQwIkkHgCCfFUoljowg4Gg8ZjCY/jggULThOWpaWlJuAvBq8vX77cdHNXVlaaWkGE3nLLLZKbm9ulli5xS0npdF1JoogzwLuQ4QcSGH4EINrQgEUXsx0TVhErKSmRSCRiemSuvvrqhBUzI90lTSGKyoQBZUYkQAIJJ5AwUYlubYhKdE1hoPqZuqfQ7Y2XlcrKysxmWlqa4NU9wYTSjHanws8kQAJ2JHDo0CF57733zFKKmLQIb+X06dMTUtSMdLe0htUeqkFkiLWEIGUmJEACCSaQMFF54MABM/Mb4ykx6aan2d8DLTsMKI3oQOnxeyRAAskigF4YDOu5+OKLBUvXYmxlwkSlLvld35asmvA6JEACJNB/AqcPfOx/HuYbmHSzZMkSs27tzp07jcdygFmd9jV6Kk9Dwh0kQAI2ImBNIJo5c6aZgPjb3/5WPvroI1m6dGnCSunGijqhDk9lwjJlRiRAAiSQQAIJ81QePXrUjKdE2dAFBCObqJS4nBJVIuZDAiRAAqcIYLjPCy+80NmYtsIK7d+/XyA0E5F06W/xJNCuJqJMzIMESIAE4gkkTFQuXrzYhBTCAHVM0kE3eEISFWVCMDITEiCBwSOAcGd33HFHny+Anp2KigopLi6WkSNHmlBs+DLCrmEoEYYPjR8/vosdRSQMdC1RV/YZM08kARJIMoGEdX83NzfL/fffLz/+8Y8lOzv7jBN1+ls/aErqyv5S4/kkQAJ2JXD48GF58sknTW/Oyy+/bMKqoazwdj733HMmvBrGY3744YddqhCJhTtEZZe9/EACJEAC9iGQMFGJMEJorSM0EAIAYxZ4olI7J+okCiXzIQESGGIC6BIvKBhhQiNhBR6sF26lESNGmDicu3fvNlEy4ocRVdce1cmQVRJq42wdixffSYAEThFATzGGItbW1p7ameStBPVRi1kJB61vCEvEkktY97cCcSVM+iaZLi9HAiRAAt0IIBZvTc16CYVCZjGIuXPnmjOwKhnE5A9/+EPzo7By5UojPK2vt4RaxNUeUo8m+24sJnwnARI4RQCN0HA4bOLkntqb3K2EiUoMTEf4DIwJamlpSehEHYrK5D4UvBoJkMDgEZg0aZJZaQeicdy4cYLPmzdvFngtr7zySlm1apW5+LXXXtulEJlZ6eL1lIk/EOyynx9IgARIAAQwFnvUqFFmcYjuy2Ani1DCRCXCZ4wZM8YISgT9xfigRK3V7fYkCwevQwIkQAKDSwC9OJjMGJ+sGeJz5swRvHpKbbE2cXvbOVGnJzjcRwIkYAsCCetYhqdy9OjRgvFC1tKLiaqhx5OwYiaqSMyHBEiABJJKIBRtkxhXgUgqc16MBEigfwQS5qnEGrcuNXhY3xspkWMqEZ+NiQRIgASGM4FQpFU8AdHxUu0S8NMoDudngXUnAbsSSJioxKxvpIwMXUsswUnn/nCZxgQzZXYkQALOItAWbZWAisqW1nbJSKeodNbdY2lJYHgQcES/Mnt8hsfDyFqSAAmcmUAoEpKgugFaVVQykQAJkIAdCThCVLL7246PDstEAiSQTALwVGb42ykqkwmd1yIBEugXAUeISg97evp1U3kyCZBA6hFo0zGV6UGXdn+nXt1YIxIggdQg4AhR6WVIodR42lgLEiCBARNoCbdKpo6lbGqODTgPfpEESIAEBpOA7UUlnJRe25dyMG8R8yYBEiABHUsZaZGcHJccr+OYSj4PJEAC9iRgf7mmqpJhKu358LBUJEACySPQpnEqC/JFKmspKpNHnVciARLoDwH7i0qtDbu/+3NLeS4JkEAqEmhTT2VBgVtqG2NcVScVbzDrRAIpQMARopKzv1PgSWMVSIAEzolAq46p9PlcEmkWispzIskvkwAJDBYB24tKjKn0JSxE+2BhZL4kQAIkMLgEouGQLgLRLjCHMc7VGVzYzJ0ESGBABGwvKlGrk4v1DKiC/BIJkAAJpASBdnR7RwUjKts5rDIlbikrQQKpRiCpPsCWlhZ5++23DcNIJCK33HKLduf4tNUdky+++EIqKirMMo+LFy+W7OzsTtZeBqrsZMENEiABZxMIh8Py5ZdfypEjRyQ/P19g7zxYi1ZTVVWVbNq0SQOct8q8efOkpKSkS2Wj7WGJuv0SVU+lr8sRfiABEiCBoSeQVE/ltm3bjIiEmPT7/bJz505DoLa2Vh577DFjWLF2eDAYPEWGs79PseAWCZCA4wns2rVLPvvsM1m2bJns2bNHYBeRIDafffZZYyMnTpwoubm5Xeuq69WGo2FJzxRpamL/d1c4/EQCJGAHAkkVlaFQyIhJlxrH9PR0aWtrMwyam5sFL7TM4c1cs2bNKTbaz1Nff1QgPNvZ53OKC7dIYBgSgM2orKyUxsZGx9a+rq5OZ3GPMLZw9OjRcujQIVMX2D5so8F94sQJee2117rYvNb6qGzavEnSPc1SU0tR6dgHgAUngUEigB6OrVu3yt69ewfpCr1nm1RROX78eNPF/emnn8qOHTtk7NixcvDgQSkqKpLzzz/ftNirq6vV4BacKrl6KttjYdOKP7WTWyRAAsORABqWaJxGo1HHVh92r6amWr766itjB2fOnGm6vdPS0mTWrFnG1qGeaHzHp/zCPBk3oVymTsySg0coKuPZcJsESEAkEAjI5MmTjbYaKh5JFZUjR46UW2+91YwjuvPOO824SYypRMv8e9/7nkB0XnbZZTJ9+vROHi5x6biiMiM8uxvZzpO4QQIkMCwIYGjMmDFjdGWZHMfWt7S0VO6++26dgOiW5cuXS3FxsRHK+PzNb35TYCfz8vLMsXibF/SlScwVlfIxXtl5kLEqHfsAsOAkMEgEYC8wPhu2ZKhSUifqoJIYmI6XleClREIrHaKyp3RyDHtPh7iPBEiABBxHAOMl48dMohscCT8I06ZN67E+AW9Ax1RGZFS+R+pq2kxYIdrGHlFxJwmQwBARGDo529cKaw/QEIruvpaS55EACZDAoBLwu4MSjoXMCmMeDSnk4BEAg8qJmZMACQwdAduLSowq4jKNQ/eA8MokQAL2IBDwwVMZVm+mjjP3i7S2cVylPe4MS0ECJGARsL2oREE9jFNp3S++kwAJDFMCGf4MaQk36wQekTHFbtmxMzJMSbDaJEACdiXgCFHp7ogLbFeGLBcJkAAJDDqBERkjpaal2lxn1hSPrF4f4co6g06dFyABEugPAUeISo8jStkf7DyXBEiABPpHYERGoVQ1Vesyje1SPtYrB460S0Mju8D7R5FnkwAJDCYBR8g1LtM4mI8A8yYBEnACgbyMPGlobdABlRqPzu+SsUUuOVRJUemEe8cyksBwIWB/UYnZ3+z+Hi7PI+tJAiRwBgI+j1+8ApPdbiJiTJjkkf0ar5KJBEiABOxCwPaiErO/2f1tl8eF5SABEhgqAm6X24jKaHvMTNaZN8Mr2/dENLSQui6ZSIAESMAGBGwvKsGInkobPCksAgmQwJASwGoZkI9YwhGpqNAjGRpa6NAR5y5ZaSrCf0iABFKGgCNEJcdUpszzxoqQAAkMkAA8lSEJq6js6PJGaKEZ072yeTtF5QCR8mskQAIJJuAIUckVdRJ815kdCZCA4wi4RNf19fqlNdLaWfZZUztEZUsLx1Z2QuEGCZDAkBGwv6jU1jjXtx2y54MXJgESsAkBdH8XpOVJXUtdZ4ky0t0yqdwtqz5lzMpOKNwgARIYMgK2F5WYqON2418mEiABEnA+AYyJPHjwoLz77ruya9euzjGSVs3a2trknXfe0Qk4p3drF2cUS21zjXWqmbBz0UK/fPhJSJrprezkwg0SIIGhIWB7UQks9FQOzcPBq5IACSSewP79++WZZ56RefPmycqVK+XAgQOdF2ltbZUXX3zR7A+Hw537rY2xuWNkT+1u66N5H1nglgXneeXjtVy2sQsYfiABEkg6AWeISkeUMun3jhckARJwIIEjR45IWdloyc3NlenTp8u2bdtMLeCZXLVqlcyePVtKS0t7rFmpisoDdQc7J+vgJEzYueaKgKzdGJHWNo6t7BEcd5IACSSFgCPkGkMKJeVZ4EVIgASSQABisqamWkKhkOkGHzVqlLlqXV2dbNmyRT777DPzvnr1aonFTolEiM+1az6VUH2LRNu7do2np7lk6QVeeX1lSAVnEirBS5AACdiOQHNzs6xdu1Y2bdo0ZGXzDtmV+3Fhdn/3AxZPJQESsDWBSZMmSWNjo+niHjdunODz5s2bZcqUKfKTn/zEjLEsKyuTJUuWqBfy1HjyqVOnyqWXLJWNH38lzeEWyQ74OuuJ0xbP98t/PtIsm7dFZKbOCo/7aud53CABEkhdAunp6bJw4UJBA3XHjh1DUlHbi0oYRk7UGZJngxclARIYBAJer1cWLFjQJeeZM2d2+Xz11Vd3+Rz/YVbRdNlRtV3mj+maBxrf37szKL/571bxq96cMtH25j2+WtwmARJIAQKO6P72cPZ3CjxqrAIJkEAiCJxXPEfWH1532qxx5I0QQ9+53S+vvROSxqZTXeeJuC7zIAESIIHeCDhCVDL4eW+3kcdJgASGC4HcjDxpbW2WcOz02eHo2RkzyitXXeKVx59vpbAcLg8F60kCNiHgCFHJsUE2eVpYDBIggSEn4HF5JCcjXypq9vZYFtjL2TP8snCuV555tVUijDTUIyfuJAESSDwBR4jKxFebOZIACZCAMwlg8s7lEy+Xl7a8LKFIW4+VgLCcd55fxqrX8ncPN8vR6hhnhfdIijtJgAQSSSCpI7mxkgSmuiNsxqxZs2TGjBlmdiPis7311ltmxhLCbVx55ZWSlpaWyHoyLxIgARJIGQKjckdLUUahbDzypSwYvajHekFYXnWpX6ZOdMtTL7TKlUv9nBXeIynuJAESSBSBpHoqd+/ebUTlTTfdZGKxHTp0yNQDsdi+/PJLGT9+vCxatEiCwWCi6sd8SIAESCDlCHhcbrl6yrXy2YG1Eo6ePrbSqjCE5Zgyrzzw3XR59e2QrPk8LJGuIS6tU/lOAiRAAudMIKmi8ujRo1JcXCwZGRkyYcIEwcoSSB6NhXHdddcZMfnGG2+YgMBWzeDdrKiokKqqqh5nO1rn8Z0ESCD1CWAZQ9iDEydOpH5le6lhcXaJTMyfIJ9WrOnlTJFAQOT/PJAulcei8uBTzVJxMMLu8F6p8QQSIIH+EkiqqCwsLJTKykoT+Bdey5KSEkHXNzyV8FKiOxzBO7v/YPj9fvH5TgX67W8leT4JkEBqEMB4QtgDNESHewKLS8ZrMPRDG+Rg3QFp1//OloIqLG+9Pig3Xx2QV1aE5OW3dHZ4I8MOnY0Zj5EACfSPQFJFJbyTGEv5+uuvm27uESNGyIYNG3R2YkQ+/vhjefXVV81auFgP10ownPBu5ufnd1ldwjrOdxIggeFDIKAuN9iDzMzM4VPps9Q03Z8u12g3+NNfPCG1jTVnOfPUodIij/zwnqCMKnbLoxp2aNXakDbsTx3nFgmQAAkMlEBSJ+pAIJ533nnmZRV4/vz5ZvOGG26wdvGdBEiABEigjwQmjJgk987/njy+/lH59tx7pDCrqNdvBoNuuWCeXxbM8cs7q9rkkeeaZVK5V8pHu6Ws1MNVzHolyBNIgAR6IpBUT2VPBeA+EiABEiCBgRNAY70ou0juOv9ueXLdI9oVXtHn8ecYRXDN0oDcvCwofu0ef+PdkLz1QUiOHotJlN7Lgd8UfpMEhimBpHoqhyljVpsESIAEBp1AIYTlvHvl1c0vyvSiWbJk/CU6ZKh3v4FqUhlR4NaXX86f6ZO168Pyh09UWB6PybxZXll4vo5h7T2bQa8fL0ACJGB/AjQV9r9HLCEJkAAJ9ErAJTr+XGeEf2/Rn0hN0zF5bsPTZwyOfqbMAn6XLFnsl9tvDsoP706XikMx+f9+1yxrvwhJ3YkYV+c5EzjuJwESMAToqeSDQAIkQAJJJBAOh01cXoRUwwTExYsXm9nsiISBiYuIkIEZ7tiflZXV75L5PD65ZdY3ZIPOCn9k3UMyq/g8mTtqvgR9/Yv/69Nfh9tvDEp9Q0w2bI7Iax+qsKyNycg8l3h8LrlWg6lnZrjVG9rvIvILJEACKUqAojJFbyyrRQIkYE8C27dvl3Xr1sl3v/tdeeWVV8yCEHPmzDHRLaZNmyZz586V1atXG4F5ySWXDKgS6PaeWzZPphZON6vuPL7+YTmv5HyZP3qBYO1wjMPsS8JpOdluWXqh35ze2hqT5haRozUxeeHNNmlpaZeJOsGnIEfMBJ+RIzwqkPuSM88hARJIRQIUlal4V1knEiAB2xKor6+XvLw8E3u3qKhIsCgEktvtNnF6d+3aJZs3b5bvfOc7Xeqwc+dOWbNmjYnnm5OjKq4PKd2fJovHLpYFZQtkxfa35F/+8H/lgvKLdN+F4vV4tcO8b+LSuhRmjWPBs/w8t0yZ4JW2UEx27IpKY1O7vLyiTY7WtsuU8R5ZONsro0q9Am8nhKn1svLhOwmQQOIJNDc3y9atW+Xrr78Wr3do5N3QXDXxLJkjCZAACTiCQHl5uXzxxRfy1VdfyY4dO+TGG280K4ahKxwxfCEqr776amlqapK0tLTOOiHO78KFCwcU+B0C8oZpN8pVk5fJ5wc+M+GH8tPyZNLIqTJl5BRBl3l/E4RiMOCW82Z0DM1fvMAv4YjI4SMR2bwzKn/4TONf6gqS7SqWsXbFeA1XNHWiV/JzXeLV7vP+ydn+lo7nk8DwIwB7gV6PY8eOGTsyFAQoKoeCOq9JAiQwbAmUlpbK3XffbZabXL58uWARiEOHoRpq7QAAGTRJREFUDpkwQOj+HjdunFllDGMv4xM8mVhJqK9d1/HfxTa+F/QGZcm4S+XCsUuksuGIrD+4Vj7Y9b7MLJ4hk0ZMlpLsUvG6B/azAJHpV/FYPkbjXerLSrrSrlm5Z9P2iLyxolWOafe5zgeSbA1hFAi6VDi7ZHSZR0oKXTKqxMvucwsc30mgnwTwNw4bAVsxVOnUX/5QlYDXJQESIIFhRiA3N1fwstLo0aPNJkRlMpJHf3RG5YyS0uxbpDHUIDurd8qXh76QD3a+p93ifinPG6Mic4rkZxT0awxmT2WH2MzKcsuF8/3mhdV7WnRspi7jbjybTc0xOXI0Jus3RuWN90KSrsI0quMyc3M8kp/tkjydq1RW5pVcHduJhSgR3gh54sVEAiRgLwIUlfa6HywNCZAACSSNADwbWYFsnR0+z7za22PSEm6VQ7qW+KcH1kh98wkJS0yK0keowBwpZTllUpAxQj2egQF7TOFEyUh368uqplsmlGs3uX7Uy6vHFmIzpuM0Rerqojr7vF1WvB+SsI7fjEVFYio43T635OrIgMJCzSfTLWnq9SzWWJvoZg8GtHvdq93rFJ0WYL6TQNIIUFQmDTUvRAIkQAL2JoBZ41hPfFLhFPNCacPRsBxrPCr7daWeT/Z8JIebKnUMpldK04slKzNfijOLpCijUHLTdfLRAMZmWkSgAfXyJnnUHRlUoTgiv2PH3NnWWR2iMxJpl9rj7VJRGZWGxpgOH2iXdzTkEVKrjusckaGTifS7aRryqHgk8ohJbb1IkQrPUUUqSHMYCsnA4j8kkGACFJUJBsrsSIAESCCVCEAolqKrXF+YSd6ursTmUJPUttRKdXON1DZVy96anfr5hKhUE5+GLIq62nUN8kIpTFexqROCMv2ZkhHM7OxKd+PMk65E672vzPA1n070KdIxmEXqqbQSPJxI0agKzrp29XTGpEm72Gs0/BE8nDk6QaiyMiZffR2RpmaM64yZMuBr3nSX6NfMeM7iXPV+qhc0J8ulnlCPGSeKa2JSErys2D5ZdHM967rx+8wB/kMCw5AAReUwvOmsMgmQAAkMlABEYEZARaK+RueO6cwGYjNmXlHj3TxSf0Sq1cO5sW6DVKnwbGprEG+7kZPqN2yXkERMQPYynRxUkDZSCrSLPSctR1+5xhMKr6lLxacbLkzd6k18WqIOXd+FI/A6JTg7C3lyA+uaY2wnsoYobFABqvHmJapezkOHNURSc7vsPRCTD9dFdFUikUhIT2rrOFdUcBboGFFcr1Svs+dIVM/R1YxUtJ4/o2OSErrvkSBEEbcT18D5eEeyytrxif+SQOoQoKhMnXvJmpAACZDAkBGA6PPgpUIQ3s0JIyaYl1UgiE6VVSo88W/UeDwxfvO4ejzr1ct5rLlKdhzfLvWtjeIxgs9IPgnHVNipDPVrnkGvDzJTAjqZyOPymtnsfq9u6wsrBmVoXM6AJyh+t198bp+Jxel2e8y2R98xs92UU/Vm/Hrm+eqdtBKCvVvJEoH4DBGKsofCMWlo0Dqoa7Na10efO9snARWk1bra0JYdUflknc7a97iMwA61tkswzS06HFSCKi7DqLu+CjNd0qof0nQafPEol1Qfaxe/jgUt1q55r14eY0TTNCYogs27VFX79Tx8L1O79TFm1KP5M5GAHQlQVNrxrrBMJEACJJBiBDo8jRCeqFiHcPOpOMwOZovknb2ymEDUpmM7Y9qPHdPtiArNqLoVQ9E2aY40S5OK09Zwi3Z3N0hEl7uMRHVijwbJjOgLorQ11iZtkYj5Pq7k1y56L1YWOlkOyFz8F9UucbyrlNPA7X6diR6QdE+6pHvTVCh3rCqEsEwBffn1WFaxOjBNCKaA5Bfq7PYRYTPZCMcgdqXdI6E2lwRUCLa1qcBUz2VMlSkEKERji3bP7z0Y1RBMbtP9XlOrYlu76rFSUZt6RnEOxHioY7io1KkHFWNGPeoJBUF1kKo47qhFtMOle9Ij2i46AsGshjRKhwmE9fuqqTvHqGJMqk+F6kgdY5quk6ZwS+A97cmDau3DO4Ttmc4DVyYSoKjkM0ACJEACJGBrAugKx4zzc0kQi/j/5D8nPaYqIqGUTNIzdLuj41qFXEQFa6hFRVyLtOh7WAWsSiozO75Wx5K2qJiFBG3T8xp0jGmTitqAelOxDGZTuEn3h1TYxXSMqVcFIIK94z8Is5PKTN/NlbGzEl7QGPy3OihAc9VfZo9fvZbqmYVXNqJiOhwJSyA/IBkqcHP8OSomvbpPPZzGg5umwwQ8xlvrVQ8tRLdmJw0nvLKrSj26KmZ97oBs3BMx4tjrVZEe0jGmtT4VnOrV1Sn1RmKjLJo6yqneUf0PAtt81nGsWjjRy0i2enZH6/CCPA35hMZC7YmY8fwWnlyms1wD3Xt1hn6aimkjSvWrJg8rr5MIsBP7O/7BW8f5J/WxuUcd2+YsnHnm1HGBjuMm3z5858y58cgACVBUDhAcv0YCJEACJOAcAhAsHeKlQ2zEe0x7qgU8ktnBvi2H2f37pwQsjnR0+XdsqQexoxBGsHUIXIhZK4eOjQ5xqwJTvbMhfUGoYkgBhG5IvbDNYRW06vFEXlH1xmJftD1ivLmRaKMRo8gxM79JfDkIot+uIrhFxpWq5zQWMvlF1SWap3kjyH5URSvKAGGLhG5+fAeCEWWBV9SMbzXvLlGnrxzT12H1tEb1ZHXMmu/vrFLdqUK3bY8OE1DPbLN236vj2HTfa3Z6rXYV5+pp1pdfRSpEJ1YTNMJT84Yz2KsNiEBaTIcUtKvnWb3H6o5N13GsGJuK/BDfFGN3A+ppRR5t6gmORNUbq8MRoppvUMV4WK8ZVFGLoa0oT1ZAQ2C5cV11imcEJFvDUEV1PENU6wvPdlS/nxkISla6X+oa4dUOSaZ6cLM0aCpiupran1THQIMnCOzBx42xv/qO/076fFWIw5OMY1rok5UDW5yD1a2QwNWreYM4rtBBXuup55t7gSfEsMd1kA2uimt1bOMcPGe4PhLOxX9ohKAhNFSJonKoyPO6JEACJEACKUkA4gH/dySry9/63I93FbbxCeGeUjlBKIV0UhQEMyZcYezokaqoikqFqccwoQpDBiAO0xSNzwhNnemvYwIamqMquXySm6niUwexBnwxFeEqbFVfhVWMt7SoCFWP7fG6Nqmr6hDkblVofhV2PhWwrSquN1WHZGxxQIJRrxzUIQqNqk6DbhXkKlghCrO0TBgf69axBSO0veH1IcpAu9Q0RGWkDuGI6vUCwXYpGOmVKp3A5XJFjFBu1eEMHhV/GTkqgFXUR1RcF+o5GelRFbAqNHXYhX5VvckqRFX8QohDzPpUIOeoAE5PjwlGUzQ0RnQSWVTS0rVMOhQiElYPvuaBhQK8ysqlwy2CemKV69iQPSYUlUOGnhcmARIgARIgARKwCMALh/GncYpcl+5U5Xgy5eV1eOWsz6feNer9WZMqzX4mCFx4VlvV4wqvKFKrek4xNhWir6EhZjyiOJahIanq9bNPRWeLnl+jwwEWT+uIXBBWjy6WL0VqUIGLfL2ax/E6Fcga4D9Xj8FBDIcj3nWUg/HKQkRHNfTVkSoNjaUjL0J6TZ+K2Sy9VpV+zlLvrV+vd1RjtO5Sod3hwdSFBVR8+mP7zPWG4h+KyqGgzmuSAAmQAAmQAAnYlgBEGmbd42WlDsHb8QkB9ONT/Ofx8QcStA0xioRydU/WMezH9nvvqYe3+0lJ+kxRmSTQvAwJkAAJkAAJkAAJDIRAT2LSyif+WPy2dTyZ7xSVyaTNa5EACZCAEohhkoD2obl13BReHYPw4WXQcVI6CwKfPTozwdpPaCRAAiTgBAJJFZWYZbZ27Vqprq6WgoICWbx4sTGcFqidO3fKl19+KbfddlvKGlP8kCDhByMVEn4E2zSgWjDYdUC5k+uGOuFZ9WOJjRRJIQ10l0r1cfJtaWpqkueff95UAffl3nvvNX8/eO4++OAD2bVrlxGdV1xxhUyePNnJVe217HV1dZKbm9vreU46oRXTkzWlkk1EfVAv/H5lZGgE9hRKqfgMDuXtSaqohLE8dOiQLF++XJ555hkZNWqUjBs3zrTOjxw5IuvWrZMDBw4MJY9Bv3ZlpQYk04S6p0KCkfn4448FP4CpklCnTZs2ybx581KlSqaxtnDhwpSpj5Mrsm3bNm00u+Wee+6WFStWGLt38cUXS3Nzs2zevFkeeOABnanaIo888ohMnDjReDJRX3g38UqltHHjRkHdUynhtwxpzJgxqVQtQb3wjE6dOjWl6pWKzyAaqEOVkioq0SLIzy8Qn88no0ePlpqaGiMqT5w4IStXrpQlS5aYBxeG0/Lk4QcexjVVWn21tbXmXufn5w/VPU/odXGvtmzZIrt3705ovkOZGeq0b98+Wb9+/VAWI6HXRoMOvQCpkvBDcOGFF6ZKdXqtB+zFJ598klJ/Z6h0Y2Oj8c72CsBBJ8D7jJRqPQOoF2xjqvwWW49UKj6D0FRD5URIqqgsLS2VDz/8UOrr62Xr1q3yjW98w4wfgtiEeETXD/ajG9xqDd1zzz3mGMcWWX8C9nufNm2a/Qp1jiVKtTpNmTLlHInY6+uTJk1ybDccbNtXX30ljz32mBlmcdlll8nnn38uc+bMkZkzZ8pDDz1kfryvu+66Ti8l6osXEwmQAAnYmYBL3aSn+Ukh4Lrvxj4YvnNJyBMeE3i10K0DbyW2rR9wDFDfvn27zJgxw4ypxPmHDx/WYKghc64XofcdllAHdHmj5WB5aDFeD10JaWlpAqHtFMHc0NAgVVVVkpOTIyNHjjQNgoqKCg1S65WSkhJTP3hi8SoqKpLMzExb1w3PG+qDBg2eRXTtYPgF7kdeXp6pI7oh8Qymp6dLcXFx54+8HR9D61lDy7uwsFCys3VNZU2oI+pWVlZmvCeoE4ahYCwbxjbb+fmznif8rYA/yrpnzx5zz7APw0hQb9QHXhTUEX9n55oWLFjQow3EteITyjMQu4iy4rmzJurgM7aRuk/U6W0senx57L6Nv68//vGP5tmcMGGC6SLGOHuMM8Xn6dOn2/p5jOeLv6MNGzaYnpr77rvP2EH02uD5xN8VniHcy1WrVpnfMKfUD+VHLw1sOoYmwNGDZxz2YtasWVJeXm7qjPNg41HPrKyseDS228bvL+qEe4bfrvnz5xsvOeoF24hGNxpt0B9wasGGXHDBBbYf74vGKf6m0LOL4Vqo25tvvmnmOaB3A/NWYLPwN4e5DxiSMXv27AH/jfXFLnYNtDTIjwIMMG7cNddcY0RlQJdOsowIjuFGoqWObSQ8BDA4EGUvv/yyI8cT4ccCZUd3Kn5EMNj52WefNd7azz77zBilQcaesOzhUYaBef31182D+tJLLxnBgn1vvfWWEV84Bk/073//ezl+/HjCrj0YGeGP7ODBg/Lkk0+a7DHW7bXXXjPeI/wh4n49/vjjxvjgXn3xxReDUYyE5Yky428FIuTRRx81zxrGhn700UemEfPqq6+aY0899ZQG7m0QfMZzaed09OhRU2YYz3feecf8SKP8qKMl8N59913zQ4CGGibA4G/O7gkCEvbOmuFtvVt2EA01yw7ihw4Ng+uvv948r3v37rV79c5YPtgEa6ImRAsEF+p59dVXy9tvv22e2TN+2WYH0L0NkWX1tOFZxfhy1AV/h/j9Qp3Q0Fm2bJmsXr3anGuzapxWHDg68LsMcYWERjXuGRwFEC2wmxiuhnribxD2xe4JToFFixaZ+wCBhd9hiC905V900UWm1wD3EfswPwD37A9/+IPdq2UaYtBTY8eOlffff9/cDzjuYD8gIGFj8DcGRxDOw2/YsWODu9pOUkVlf+4QHlYAwA1HqwJ/sPiRd1qCwYRXFuNH8TDv2LHDeCjnzp2rdbvYGB7rx9HudYM3D15l1An3Ag8nWi64R/ihg9cZf4yoGx5ytGTtnDCLEY0YGBykESNG6JjffCP0P/30U+M9gQE977zzTNek3QUYhAq6UOHRw/2BYcF9wQ8fDCq8eWixw3iitYoXJsfZOaGrGJP58CONe4M6lqunBONDMZQGXgb8TWH8EJ5FiBYn2omz3QM00vCjgPuJH3bcP6cmzGb//ve/b37wnnjiCVMX1Ak/fqifNXPaCfVDQwA2xPIwo4cAHjs4S3Jyck2PDWwkeg0gXuDVg6fM7gllRR2shLHLuGe4RxiagTrgXuEz7h3+Bu2eUFbUCzYD9hANAjSs4VFGjw5+0+Atx28x6g5b44R64bcL2ggNboh81ANe88svv9zoja+//tocx28C6oy6Dna9bN2fjB95ALMGPFt/vHZ/gOPLh5t8ySWXmAcVLYZ96hnCwwzjWVVVaYRM/Pl23sYfnPVCvZDwh4gfOXRFossYP/AQYvAaQVzaOVl1sd7hOcEfJLxg8LSixYruKwy/QDesJT7tWifUA14tePe/973vGcOPHz00aPDDBsOK+4S/I9QJQg3Gxs4JZcRYa3TtoAGAOt51113GG/nggw8aoQxDiWfQ8lA60U6c7R6Uq4iGNxYzw/H39c1vfvNsp9v6GEQ/7hXsBp5NNBjgPMDfnCW8bF2BuMLBNuzfv9/YBvRywDMEu476HDp00Nh9/H3BQ4lGNuwixIrdE2wdHAT47UVDFPW07Dz+1nDfcK/Qe4CeEGv4mp3rhTo9/PDDxg6iAQpbAW8syg/7iPsEvYHfMDRY4Z3F353dE+wCvOM33HCDeb5g19GrgXsGOwiBjN9hOEnwdwenArzNg5mSOqayvxWB+LK6I5cuXWr+MC0x09+8hup8eE1QB9xMeBvQgsAPP34g8Id50003mRbsUJWvP9fFeKH33nvP1AWTC+CVhPcVDy+EM8a34SGHEICBxTlozds1wVCiux6tOXj44EWB5w6tcBgc7MNn3CsYUrQE7RxTD4bkV7/6lRmnaz1bqAu6qvC3hOgKGNcFo4mxYPiBw2QQq9Fmx/uEyA/4QYCxh9cS463feOMN442E5xxeFHiDIDzxQ4HPGGJzrnaiL2OHwAvXGciYyv6yxg8FGmromsSPwrnWr7/XT9T5EJUYh40fO9w/NHJQL/ywjx8/3jG2EDwghCG+YEdwPyCuLKGJewQvHhpB6C2AdwjC0u5jD1Ev/D3hHiFBaFn3CDYQdh32Al5Za+woGuN2b8jB/qE7H/fJuldoZGP4E+pSrgISTgPcJ9QL27hfdv79wv1BIxPPHxLuFe4F7h3qC1uBfUjYBw2COkGHDDT1xS7aWlQOtOL8HgmQAAmcC4G+GE/kjx+oZIjKc6kLv0sCJEACiSDQF7to2zGViQDAPEiABEiABEiABEiABJJDgKIyOZx5FRIgARIgARIgARJIaQIUlSl9e4dH5TCWDoPGU23W7/C4e6wlCZDAYBBAqB+sPMVEAskkYOvZ38kEwWsNHQEMZocghDjExBJ8xjYSPmPcGo7jhc8YFI5zMCgeCccRowuhmzD42vqOOch/SIAESMBhBDAJCJMEMekR49gw8QKTnDCpCbOXMUkS24ifi7A4CNQNO7hmzRqzH9ESMNsXE7wQtxDhwzBJD+cwkcBgEqCoHEy6zLtXAhCHCOsAw4cZoYiBhrAIeGEGHgTiVVddJc8995yZzYaZiXfccYf8QQPT4lzMCsYsbWyjZb5PQzZdeeWVxoD2enGeQAIkQAI2IwCb+Morr5hSQQhisQzM2EW0BtjC//mf/xEsX4zFC+68804zAxgLbKCxjZVhEJMWDXDYVdjRG2+80YTT+dGPfmTsqc2qy+KkGAF2f6fYDXVidRC/Ey1zGFMYQ8R6Q8B7GFCsSoFwCAjlgxUBEP4BrW+s4nPttdeaUEYIKgxBihBGaLHbPei6E+8Ry0wCJJA8ArCDEImwa2gkW2G/YCPxshI8j/Hex/ht9NqUnwyVg9A48d+zvs93Ekg0AYrKRBNlfv0iACMIsYgWNbp4EN8NhhTxL59//oXO2Iro+nn66aeNuET8LXTv4DPiTCL+Fr6DBOMZb1j7VRieTAIkQAJDTAD267bbbjNxPBFDFw1uxLeEyEQc4Ntvv93EH7zllltMfFY0wpcvX25eiOkKLye6xhF3F7FAkR+6zPF9JhIYbAKMUznYhJl/rwQsDyXeYfggFBF8HKttWOMj0XLHOEuIRpxjfQeZYx+OWfuxjwYUFJgGSqAv8diQN36wGadyoJT5vb4SwHAfNKYhFJlIYKgI9MUuckzlUN0dXreTAH6YIR6tdPHFF5sVbNB9YyUIR7ys1P071jHsZyIBEiCBVCKwdOnSVKoO65LCBE79kqdwJVk1ZxGwlpZyVqlZWhIgARIgARIY3gQ4yGJ433/WngRIgARIgARIgAQSQqBfYyoTckVmQgIkQAIOINB9tiyGVvS0zwFVYRFJgARIICEEerKB8fv63P0d/6WElIyZkAAJkIDDCdAuOvwGsvgkQAIJJcDu74TiZGYkQAIkQAIkQAIkMDwJUFQOz/vOWpMACZAACZAACZBAQglQVCYUJzMjARIgARIgARIggeFJ4IxjKhnvb3g+EKw1CZDAmQnQLp6ZDY+QAAmQQI+ikoPP+WCQAAmQAAmQAAmQAAn0hwC7v/tDi+eSAAmQAAmQAAmQAAn0SOD/B2smgmLBH680AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The data for 20 epochs: \n",
    "\n",
    "- accuracy: 0.9461\n",
    "- Test score: 0.19061198830604553\n",
    "- Test accuracy: 0.9460999965667725\n",
    "- 60000 train samples\n",
    "- 10000 test samples\n",
    "\n",
    "The data for for 60 epochs: \n",
    "\n",
    "- accuracy: 0.9674\n",
    "- Test score: 0.10819873213768005\n",
    "- Test accuracy: 0.9674000144004822\n",
    "- 60000 train samples\n",
    "- 10000 test samples\n",
    "\n",
    "For 120 epochs: \n",
    "\n",
    "- accuracy: 0.9752\n",
    "- Test score: 0.08114946633577347\n",
    "- Test accuracy: 0.9751999974250793\n",
    "- 60000 train samples\n",
    "- 10000 test samples\n",
    "\n",
    "This code was provided in the text, Deep Learning With Keras (Gulli & Pal, 2017). The code is an algorithm used to train the computer to recognize handwritten digits using supervised learning. According to IBM, supervised learning uses labeled datasets to train models to classify data or reach a certain conclusion (What Is Supervised Learning?  | IBM, n.d.). \n",
    "\n",
    "In this code, we can observe that with each iteration or epoch, the test accuracy increases as the machine becomes experienced. As expected, we see that as we increase the number of epochs, we observe that the accuracy increases. Though I only tested up to 120 epochs, Gulli and Pal explain that as we test up to a certain point, the accuracy increases, but after we test beyond around 100 epochs, increases in accuracy begin to slow. In the figure on page 28, we see that as we train the data past approximately 100 epochs, there is a point at which the accuracy benefits with additional training begin to diminish (Gulli & Pal, 2017). \n",
    "\n",
    "![image.png](attachment:image.png) \n",
    "\n",
    "Image credit: (Gulli & Pal, 2017)\n",
    "\n",
    "\n",
    "In these cases, the model may be more likely to overfit the data, making generalizations that are not beneficial for predicting digits correctly. According to *Do Machine Learning Models Memorize or Generalize?* from the website Google PAIR, a “model overfits the training data when it performs well on the training data but poorly on the test data” (n.d.). Similarly, Investopedia defines overfitting as a “modeling error in statistics that occurs when a function is too closely aligned to a limited set of data points” (Twin, 2021) Overfitting may also involve making generalizations based on characteristics that are not appropriate, which is what we see in this model. \n",
    "\n",
    "In this model, accuracy rates increase as the training sample size and validation are kept constant. This happens as the model improves and \"learns\" patterns over many iterations. As we manipulate the number of epochs that the model is trained over, we increase the iterations that the model will execute the full dataset over and for each iteration, the model gains experience. As the data grows, the model becomes more and more experienced and efficient with its prediction capabilities. The benefits, however, begin to taper off and only modest improvements can be achieved after a certain number of epochs (around 100). After this point, the model begins overfitting and making mistakes and generalizations that prevent it from significantly improving in accuracy any further. \n",
    "\n",
    "**References**\n",
    "\n",
    "*Do machine learning models memorize or generalize?* (n.d.). https://pair.withgoogle.com/explorables/grokking/#:~:text=A%20model%20overfits%20the%20training,required%20to%20make%20more%20generalizations.\n",
    "\n",
    "Gulli, A., & Pal, S. (2017). *Deep Learning with Keras. Packt Publishing Limited.* ISBN: 978-1-78712-842-2.\n",
    "\n",
    "Twin, A. (2021, October 22). *Understanding overfitting and how to prevent it.* Investopedia. https://www.investopedia.com/terms/o/overfitting.asp\n",
    "\n",
    "What is Supervised Learning?  | *IBM.* (n.d.). https://www.ibm.com/topics/supervised-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
